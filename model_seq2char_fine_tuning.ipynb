{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c2d31ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "GPU Name:  NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Using device: cuda\n",
      "Tensor on GPU: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Exemplo rápido para confirmar que está usando a GPU\n",
    "x = torch.rand(5, 5).to(device)\n",
    "y = torch.rand(5, 5).to(device)\n",
    "z = x @ y\n",
    "print(\"Tensor on GPU:\", z.is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5779cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7858e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading 'punkt' NLTK package...\")\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    print(\"Downloading 'wordnet' NLTK package...\")\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a46cde",
   "metadata": {},
   "source": [
    "## Separação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77933d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Realiza o pré-processamento completo para um fluxo de pré-treinamento e\n",
    "fine-tuning, tudo em uma única execução. O processo é dividido em três fases:\n",
    "(1) Isola as letras do artista alvo em conjuntos exclusivos de treino,\n",
    "validação e teste para evitar vazamento de dados. (2) Constrói um corpus geral\n",
    "com as letras de todos os outros artistas (e as letras não exclusivas do\n",
    "artista alvo), cria e salva um tokenizador geral e os datasets tokenizados\n",
    "correspondentes. (3) Utiliza o tokenizador geral para processar e salvar os\n",
    "datasets exclusivos do artista alvo, deixando-os prontos para o fine-tuning.\n",
    "\"\"\"\n",
    "\n",
    "CSV_FOLDER_PATH = 'csv'\n",
    "CHUNK_SIZE = 10000\n",
    "\n",
    "SEQUENCE_LENGTH = 100 \n",
    "\n",
    "# Diretórios base de saída\n",
    "BASE_OUTPUT_DIR_GENERAL = 'processed_data_general_char_split'\n",
    "BASE_OUTPUT_DIR_ARTIST = 'processed_data_by_artist_char_split' \n",
    "\n",
    "TOKENIZER_SUBDIR = 'tokenizers'\n",
    "DATA_SUBDIR = 'data'\n",
    "\n",
    "TARGET_ARTIST = 'ArianaGrande' \n",
    "NUM_SONGS_TO_PROCESS = -1\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1 \n",
    "RANDOM_SEED = 42\n",
    "\n",
    "CLEAN_TARGET_ARTIST_NAME = TARGET_ARTIST.replace(\" \", \"_\").replace(\".\", \"\").lower()\n",
    "\n",
    "# Frase placeholder a ser removida\n",
    "PLACEHOLDER_PHRASE = \"lyrics for this song have yet to be released please check back once the song has been released\"\n",
    "\n",
    "\n",
    "# --- Funções Auxiliares ---\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z0-9\\s.,!?;:\\'\\-]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "def save_object(obj, path):\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"Objeto salvo em: {path}\")\n",
    "\n",
    "def load_object(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'rb') as handle:\n",
    "            obj = pickle.load(handle)\n",
    "        print(f\"Objeto carregado de: {path}\")\n",
    "        return obj\n",
    "    return None\n",
    "\n",
    "class CharacterTokenizer:\n",
    "    def __init__(self):\n",
    "        self.char_to_int = {}\n",
    "        self.int_to_char = {}\n",
    "        self.vocab_size = 0\n",
    "    def fit_on_texts(self, texts):\n",
    "        if isinstance(texts, list): all_chars = \"\".join(texts)\n",
    "        else: all_chars = texts\n",
    "        unique_chars = sorted(list(set(all_chars)))\n",
    "        self.char_to_int = {char: i + 1 for i, char in enumerate(unique_chars)}\n",
    "        self.int_to_char = {i + 1: char for i, char in enumerate(unique_chars)}\n",
    "        self.vocab_size = len(self.char_to_int) + 1\n",
    "    def texts_to_sequences(self, texts):\n",
    "        if isinstance(texts, str): return [self.char_to_int.get(char, 0) for char in texts]\n",
    "        elif isinstance(texts, list): return [[self.char_to_int.get(char, 0) for char in s] for s in texts]\n",
    "        return []\n",
    "    def sequences_to_texts(self, sequences):\n",
    "        if isinstance(sequences[0], list): return [\"\".join([self.int_to_char.get(i, '') for i in seq]) for seq in sequences]\n",
    "        else: return \"\".join([self.int_to_char.get(i, '') for i in sequences])\n",
    "\n",
    "def create_sequences_from_lyrics_list(lyrics_list, tokenizer, sequence_length, description):\n",
    "    all_sequences = []\n",
    "    all_next_chars = []\n",
    "    print(f\"\\n{description}: Criando sequências e tokenizando...\")\n",
    "    total_chars_in_subset = 0\n",
    "    for lyric in tqdm(lyrics_list, desc=f\"Processando {description} músicas\"):\n",
    "        tokenized_lyric = tokenizer.texts_to_sequences(lyric)\n",
    "        total_chars_in_subset += len(tokenized_lyric)\n",
    "        if len(tokenized_lyric) < sequence_length + 1:\n",
    "            continue\n",
    "\n",
    "        for i in range(0, len(tokenized_lyric) - sequence_length):\n",
    "            seq = tokenized_lyric[i : i + sequence_length]\n",
    "            next_c = tokenized_lyric[i + sequence_length]\n",
    "            all_sequences.append(seq)\n",
    "            all_next_chars.append(next_c)\n",
    "\n",
    "    print(f\"Total de caracteres processados no {description}: {total_chars_in_subset}\")\n",
    "    print(f\"Total de sequências criadas para {description}: {len(all_sequences)}\")\n",
    "\n",
    "    if not all_sequences:\n",
    "        print(f\"Nenhuma sequência válida criada para {description}. Verifique o comprimento das letras ou SEQUENCE_LENGTH.\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    return np.array(all_sequences, dtype=np.int32), np.array(all_next_chars, dtype=np.int32)\n",
    "\n",
    "\n",
    "# --- FASE 1: PRÉ-PROCESSAMENTO DO ARTISTA ALVO (PARA ISOLAR TESTE, TREINO E VALIDAÇÃO EXCLUSIVOS) ---\n",
    "print(\"--- FASE 1: PRÉ-PROCESSAMENTO DO ARTISTA ALVO (PARA ISOLAR TESTE, TREINO E VALIDAÇÃO EXCLUSIVOS) ---\")\n",
    "\n",
    "ARTIST_LYRICS_CLEAN_ALL = [] \n",
    "TEST_LYRICS_ARTIST_FINAL = [] \n",
    "VAL_LYRICS_ARTIST_EXCLUSIVE = [] \n",
    "TRAIN_LYRICS_ARTIST_EXCLUSIVE = [] \n",
    "\n",
    "CSV_FILE_PATH_TARGET_ARTIST = os.path.join(CSV_FOLDER_PATH, f'{TARGET_ARTIST}.csv')\n",
    "\n",
    "print(f\"Coletando e limpando TODAS as letras de '{TARGET_ARTIST}' do arquivo '{CSV_FILE_PATH_TARGET_ARTIST}' para separação...\")\n",
    "try:\n",
    "    df_target_artist_full = pd.read_csv(CSV_FILE_PATH_TARGET_ARTIST, usecols=['Lyric'])\n",
    "\n",
    "    initial_lyrics_count_artist_all = len(df_target_artist_full)\n",
    "    removed_lyrics_count_artist_all = 0\n",
    "\n",
    "    for lyric_val in tqdm(df_target_artist_full['Lyric'], desc=f\"Limpando e filtrando letras de {TARGET_ARTIST}\"):\n",
    "        cleaned_lyric = clean_text(lyric_val)\n",
    "        \n",
    "        if PLACEHOLDER_PHRASE in cleaned_lyric:\n",
    "            removed_lyrics_count_artist_all += 1\n",
    "            continue\n",
    "        \n",
    "        if cleaned_lyric:\n",
    "            ARTIST_LYRICS_CLEAN_ALL.append(cleaned_lyric)\n",
    "    \n",
    "    print(f\"Total de letras iniciais para '{TARGET_ARTIST}': {initial_lyrics_count_artist_all}\")\n",
    "    print(f\"Total de letras removidas (placeholder) para '{TARGET_ARTIST}': {removed_lyrics_count_artist_all}\")\n",
    "    print(f\"Total de letras coletadas e limpas de '{TARGET_ARTIST}': {len(ARTIST_LYRICS_CLEAN_ALL)}\")\n",
    "\n",
    "    random.seed(RANDOM_SEED) \n",
    "    random.shuffle(ARTIST_LYRICS_CLEAN_ALL)\n",
    "\n",
    "    num_total_artist_lyrics = len(ARTIST_LYRICS_CLEAN_ALL)\n",
    "    \n",
    "    num_test_artist_final = int(num_total_artist_lyrics * TEST_SPLIT)\n",
    "    num_val_artist_exclusive = int(num_total_artist_lyrics * VAL_SPLIT)\n",
    "    num_train_artist_exclusive = num_total_artist_lyrics - num_test_artist_final - num_val_artist_exclusive\n",
    "\n",
    "    if num_train_artist_exclusive < 0:\n",
    "        num_train_artist_exclusive = 0 \n",
    "\n",
    "    TEST_LYRICS_ARTIST_FINAL = ARTIST_LYRICS_CLEAN_ALL[:num_test_artist_final]\n",
    "    VAL_LYRICS_ARTIST_EXCLUSIVE = ARTIST_LYRICS_CLEAN_ALL[num_test_artist_final : num_test_artist_final + num_val_artist_exclusive]\n",
    "    TRAIN_LYRICS_ARTIST_EXCLUSIVE = ARTIST_LYRICS_CLEAN_ALL[num_test_artist_final + num_val_artist_exclusive : num_test_artist_final + num_val_artist_exclusive + num_train_artist_exclusive]\n",
    "    \n",
    "    print(f\"\\nDivisão das músicas de '{TARGET_ARTIST}' (Exclusivas para Fine-tuning):\")\n",
    "    print(f\"Músicas reservadas para TESTE FINAL: {len(TEST_LYRICS_ARTIST_FINAL)} músicas\")\n",
    "    print(f\"Músicas reservadas para VALIDAÇÃO EXCLUSIVA: {len(VAL_LYRICS_ARTIST_EXCLUSIVE)} músicas\")\n",
    "    print(f\"Músicas reservadas para TREINO EXCLUSIVO: {len(TRAIN_LYRICS_ARTIST_EXCLUSIVE)} músicas\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo CSV do artista alvo não encontrado em '{CSV_FILE_PATH_TARGET_ARTIST}'. Não será possível evitar vazamento de dados.\")\n",
    "    exit()\n",
    "except ValueError as e:\n",
    "    print(f\"Erro na separação de dados de '{TARGET_ARTIST}': {e}. Não será possível evitar vazamento de dados.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- FASE 2: PRÉ-PROCESSAMENTO PARA O MODELO GERAL (TODOS OS ARTISTAS, EXCLUINDO DADOS EXCLUSIVOS DO ARTISTA ALVO) ---\n",
    "print(\"\\n--- FASE 2: PRÉ-PROCESSAMENTO PARA O MODELO GERAL (Excluindo dados exclusivos do artista alvo) ---\")\n",
    "\n",
    "ALL_ARTISTS_LYRICS_FOR_GENERAL_MODEL = [] \n",
    "GENERAL_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR_GENERAL)\n",
    "os.makedirs(os.path.join(GENERAL_OUTPUT_DIR, TOKENIZER_SUBDIR), exist_ok=True)\n",
    "os.makedirs(os.path.join(GENERAL_OUTPUT_DIR, DATA_SUBDIR), exist_ok=True)\n",
    "\n",
    "EXCLUSIVE_ARTIST_DATA_SET = set(TEST_LYRICS_ARTIST_FINAL + VAL_LYRICS_ARTIST_EXCLUSIVE + TRAIN_LYRICS_ARTIST_EXCLUSIVE)\n",
    "\n",
    "for filename in os.listdir(CSV_FOLDER_PATH):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        artist_name_current_file = filename.replace(\".csv\", \"\")\n",
    "        csv_file_path_artist = os.path.join(CSV_FOLDER_PATH, filename)\n",
    "        \n",
    "        try:\n",
    "            df_artist_current = pd.read_csv(csv_file_path_artist, usecols=['Lyric'])\n",
    "            \n",
    "            for lyric_val in tqdm(df_artist_current['Lyric'], desc=f\"Coletando corpus geral de {artist_name_current_file}\"):\n",
    "                cleaned_lyric = clean_text(lyric_val)\n",
    "                \n",
    "                if PLACEHOLDER_PHRASE in cleaned_lyric:\n",
    "                    continue\n",
    "                \n",
    "                if artist_name_current_file == TARGET_ARTIST and cleaned_lyric in EXCLUSIVE_ARTIST_DATA_SET:\n",
    "                    continue \n",
    "                \n",
    "                if cleaned_lyric:\n",
    "                    ALL_ARTISTS_LYRICS_FOR_GENERAL_MODEL.append(cleaned_lyric)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao coletar letras de '{artist_name_current_file}' para corpus geral: {e}. Pulando.\")\n",
    "\n",
    "print(f\"\\nTotal de letras no Corpus GERAL (APÓS exclusão de todos os dados exclusivos do artista alvo): {len(ALL_ARTISTS_LYRICS_FOR_GENERAL_MODEL)}\")\n",
    "\n",
    "\n",
    "random.seed(RANDOM_SEED) \n",
    "random.shuffle(ALL_ARTISTS_LYRICS_FOR_GENERAL_MODEL)\n",
    "\n",
    "# Dividir o corpus geral para treinamento do modelo base\n",
    "num_total_lyrics_general = len(ALL_ARTISTS_LYRICS_FOR_GENERAL_MODEL)\n",
    "num_train_general = int(num_total_lyrics_general * TRAIN_SPLIT)\n",
    "num_val_general = int(num_total_lyrics_general * VAL_SPLIT)\n",
    "num_test_general = num_total_lyrics_general - num_train_general - num_val_general\n",
    "\n",
    "train_lyrics_general = ALL_ARTISTS_LYRICS_FOR_GENERAL_MODEL[:num_train_general]\n",
    "val_lyrics_general = ALL_ARTISTS_LYRICS_FOR_GENERAL_MODEL[num_train_general : num_train_general + num_val_general]\n",
    "test_lyrics_general = ALL_ARTISTS_LYRICS_FOR_GENERAL_MODEL[num_train_general + num_val_general :] \n",
    "\n",
    "print(f\"\\nDivisão das músicas (Corpus GERAL FINAL):\")\n",
    "print(f\"Treino Geral: {len(train_lyrics_general)} músicas\")\n",
    "print(f\"Validação Geral: {len(val_lyrics_general)} músicas\")\n",
    "print(f\"Teste Geral: {len(test_lyrics_general)} músicas\")\n",
    "\n",
    "# Criar e ajustar o Tokenizador Geral\n",
    "lyrics_for_vocab_general = \" \".join(train_lyrics_general)\n",
    "print(\"\\nPasso 4: Construindo vocabulário de caracteres GERAL com as letras de TREINO GERAL...\")\n",
    "tokenizer_general = CharacterTokenizer()\n",
    "tokenizer_general.fit_on_texts(lyrics_for_vocab_general)\n",
    "total_chars_general = tokenizer_general.vocab_size\n",
    "print(f\"Vocabulário de caracteres GERAL construído. Tamanho do vocabulário: {total_chars_general}\")\n",
    "\n",
    "TOKENIZER_SAVE_PATH_GENERAL = os.path.join(GENERAL_OUTPUT_DIR, TOKENIZER_SUBDIR, f'char_tokenizer_general.pkl')\n",
    "save_object(tokenizer_general, TOKENIZER_SAVE_PATH_GENERAL)\n",
    "\n",
    "# Criar sequências e tokenizar para o conjunto GERAL (Treino, Val, Teste)\n",
    "X_train_general, y_train_general = create_sequences_from_lyrics_list(train_lyrics_general, tokenizer_general, SEQUENCE_LENGTH, \"Conjunto de Treino Geral\")\n",
    "X_val_general, y_val_general = create_sequences_from_lyrics_list(val_lyrics_general, tokenizer_general, SEQUENCE_LENGTH, \"Conjunto de Validação Geral\")\n",
    "X_test_general, y_test_general = create_sequences_from_lyrics_list(test_lyrics_general, tokenizer_general, SEQUENCE_LENGTH, \"Conjunto de Teste Geral\")\n",
    "\n",
    "print(f\"\\nShapes finais dos dados tokenizados (Corpus Geral):\")\n",
    "print(f\"X_train_general shape: {X_train_general.shape}, y_train_general shape: {y_train_general.shape}\")\n",
    "print(f\"X_val_general shape: {X_val_general.shape}, y_val_general shape: {y_val_general.shape}\")\n",
    "print(f\"X_test_general shape: {X_test_general.shape}, y_test_general shape: {y_test_general.shape}\")\n",
    "\n",
    "# Salvar dados tokenizados do corpus GERAL\n",
    "output_file_x_train_general = os.path.join(GENERAL_OUTPUT_DIR, DATA_SUBDIR, f'X_train_general_char.npy')\n",
    "output_file_y_train_general = os.path.join(GENERAL_OUTPUT_DIR, DATA_SUBDIR, f'y_train_general_char.npy')\n",
    "np.save(output_file_x_train_general, X_train_general)\n",
    "np.save(output_file_y_train_general, y_train_general)\n",
    "print(f\"Dados de TREINO GERAL salvos em {output_file_x_train_general} e {output_file_y_train_general}\")\n",
    "\n",
    "output_file_x_val_general = os.path.join(GENERAL_OUTPUT_DIR, DATA_SUBDIR, f'X_val_general_char.npy')\n",
    "output_file_y_val_general = os.path.join(GENERAL_OUTPUT_DIR, DATA_SUBDIR, f'y_val_general_char.npy')\n",
    "np.save(output_file_x_val_general, X_val_general)\n",
    "np.save(output_file_y_val_general, y_val_general)\n",
    "print(f\"Dados de VALIDAÇÃO GERAL salvos em {output_file_x_val_general} e {output_file_y_val_general}\")\n",
    "\n",
    "output_file_x_test_general = os.path.join(GENERAL_OUTPUT_DIR, DATA_SUBDIR, f'X_test_general_char.npy')\n",
    "output_file_y_test_general = os.path.join(GENERAL_OUTPUT_DIR, DATA_SUBDIR, f'y_test_general_char.npy')\n",
    "np.save(output_file_x_test_general, X_test_general)\n",
    "np.save(output_file_y_test_general, y_test_general)\n",
    "print(f\"Dados de TESTE GERAL salvos em {output_file_x_test_general} e {output_file_y_test_general}\")\n",
    "\n",
    "output_file_vocab_size_general = os.path.join(GENERAL_OUTPUT_DIR, TOKENIZER_SUBDIR, f'vocab_size_general_char.pkl')\n",
    "save_object(total_chars_general, output_file_vocab_size_general)\n",
    "print(f\"Tamanho do vocabulário GERAL salvo em {output_file_vocab_size_general}\")\n",
    "\n",
    "print(f\"\\nPré-processamento para o Corpus GERAL (caractere a caractere) concluído.\")\n",
    "\n",
    "\n",
    "# --- FASE 3: PRÉ-PROCESSAMENTO PARA O ARTISTA ALVO (FINE-TUNING) ---\n",
    "print(\"\\n--- FASE 3: PRÉ-PROCESSAMENTO PARA O ARTISTA ALVO (FINE-TUNING) ---\")\n",
    "\n",
    "ARTIST_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR_ARTIST, CLEAN_TARGET_ARTIST_NAME)\n",
    "os.makedirs(os.path.join(ARTIST_OUTPUT_DIR, TOKENIZER_SUBDIR), exist_ok=True)\n",
    "os.makedirs(os.path.join(ARTIST_OUTPUT_DIR, DATA_SUBDIR), exist_ok=True)\n",
    "\n",
    "train_lyrics_artist = TRAIN_LYRICS_ARTIST_EXCLUSIVE\n",
    "val_lyrics_artist = VAL_LYRICS_ARTIST_EXCLUSIVE\n",
    "test_lyrics_artist = TEST_LYRICS_ARTIST_FINAL \n",
    "\n",
    "print(f\"\\nDivisão das músicas (Artista Alvo '{TARGET_ARTIST}' para Fine-tuning):\")\n",
    "print(f\"Treino Artista: {len(train_lyrics_artist)} músicas\")\n",
    "print(f\"Validação Artista: {len(val_lyrics_artist)} músicas\")\n",
    "print(f\"Teste Artista: {len(test_lyrics_artist)} músicas (Exclusivo, NÃO Visto no Treino Geral!)\")\n",
    "\n",
    "print(f\"\\nReutilizando o tokenizer geral ({TOKENIZER_SAVE_PATH_GENERAL}) para os dados do artista alvo...\")\n",
    "try:\n",
    "    tokenizer_for_artist_data = load_object(TOKENIZER_SAVE_PATH_GENERAL)\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: Tokenizer geral não encontrado. Não é possível processar dados do artista alvo. Por favor, rode a FASE 2 primeiro.\")\n",
    "    exit()\n",
    "\n",
    "X_train_artist, y_train_artist = create_sequences_from_lyrics_list(train_lyrics_artist, tokenizer_for_artist_data, SEQUENCE_LENGTH, \"Conjunto de Treino Artista\")\n",
    "X_val_artist, y_val_artist = create_sequences_from_lyrics_list(val_lyrics_artist, tokenizer_for_artist_data, SEQUENCE_LENGTH, \"Conjunto de Validação Artista\")\n",
    "X_test_artist, y_test_artist = create_sequences_from_lyrics_list(test_lyrics_artist, tokenizer_for_artist_data, SEQUENCE_LENGTH, \"Conjunto de Teste Artista (Exclusivo)\")\n",
    "\n",
    "print(f\"\\nShapes finais dos dados tokenizados (Artista Alvo '{TARGET_ARTIST}'):\")\n",
    "print(f\"X_train_artist shape: {X_train_artist.shape}, y_train_artist shape: {y_train_artist.shape}\")\n",
    "print(f\"X_val_artist shape: {X_val_artist.shape}, y_val_artist shape: {y_val_artist.shape}\")\n",
    "print(f\"X_test_artist shape: {X_test_artist.shape}, y_test_artist shape: {y_test_artist.shape}\")\n",
    "\n",
    "# Salvar dados tokenizados do ARTISTA ALVO\n",
    "output_file_x_train_artist = os.path.join(ARTIST_OUTPUT_DIR, DATA_SUBDIR, f'X_train_{CLEAN_TARGET_ARTIST_NAME}_char.npy')\n",
    "output_file_y_train_artist = os.path.join(ARTIST_OUTPUT_DIR, DATA_SUBDIR, f'y_train_{CLEAN_TARGET_ARTIST_NAME}_char.npy')\n",
    "np.save(output_file_x_train_artist, X_train_artist)\n",
    "np.save(output_file_y_train_artist, y_train_artist)\n",
    "print(f\"Dados de TREINO do Artista salvos em {output_file_x_train_artist} e {output_file_y_train_artist}\")\n",
    "\n",
    "output_file_x_val_artist = os.path.join(ARTIST_OUTPUT_DIR, DATA_SUBDIR, f'X_val_{CLEAN_TARGET_ARTIST_NAME}_char.npy')\n",
    "output_file_y_val_artist = os.path.join(ARTIST_OUTPUT_DIR, DATA_SUBDIR, f'y_val_{CLEAN_TARGET_ARTIST_NAME}_char.npy')\n",
    "np.save(output_file_x_val_artist, X_val_artist)\n",
    "np.save(output_file_y_val_artist, y_val_artist)\n",
    "print(f\"Dados de VALIDAÇÃO do Artista salvos em {output_file_x_val_artist} e {output_file_y_val_artist}\")\n",
    "\n",
    "output_file_x_test_artist = os.path.join(ARTIST_OUTPUT_DIR, DATA_SUBDIR, f'X_test_{CLEAN_TARGET_ARTIST_NAME}_char.npy')\n",
    "output_file_y_test_artist = os.path.join(ARTIST_OUTPUT_DIR, DATA_SUBDIR, f'y_test_{CLEAN_TARGET_ARTIST_NAME}_char.npy')\n",
    "np.save(output_file_x_test_artist, X_test_artist)\n",
    "np.save(output_file_y_test_artist, y_test_artist)\n",
    "print(f\"Dados de TESTE do Artista salvos em {output_file_x_test_artist} e {output_file_y_test_artist}\")\n",
    "\n",
    "output_file_vocab_size_artist = os.path.join(ARTIST_OUTPUT_DIR, TOKENIZER_SUBDIR, f'vocab_size_{CLEAN_TARGET_ARTIST_NAME}_char.pkl')\n",
    "save_object(total_chars_general, output_file_vocab_size_artist) \n",
    "print(f\"Tamanho do vocabulário GERAL (para o artista alvo) salvo em {output_file_vocab_size_artist}\")\n",
    "\n",
    "\n",
    "print(f\"\\nPré-processamento para o artista '{TARGET_ARTIST}' (caractere a caractere) concluído.\")\n",
    "print(f\"Dados tokenizados para o artista salvo no diretório: {ARTIST_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592e572",
   "metadata": {},
   "source": [
    "## Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b71fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define as configurações para as fases de pré-treinamento e fine-tuning,\n",
    "incluindo diretórios de modelo, épocas e taxas de aprendizado. Também define\n",
    "as classes PyTorch essenciais: a classe 'LyricsDataset' para carregar os\n",
    "dados tokenizados de forma eficiente e a arquitetura do modelo 'CharLSTM'.\n",
    "\"\"\"\n",
    "# --- Configurações de Treinamento e Modelo ---\n",
    "MODEL_DIR_BASE_GENERAL = 'models_general_char_split'\n",
    "MODEL_DIR_BASE_ARTIST_FINE_TUNED = 'models_by_artist_char_split_finetuned'\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS_GENERAL = 5\n",
    "EPOCHS_FINE_TUNE = 10\n",
    "LEARNING_RATE_GENERAL = 0.001\n",
    "LEARNING_RATE_FINE_TUNE = 0.0001\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 4\n",
    "DROPOUT_RATE = 0.2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# --- Caminhos Específicos dos Modelos ---\n",
    "GENERAL_DATA_DIR = os.path.join(BASE_OUTPUT_DIR_GENERAL)\n",
    "ARTIST_DATA_DIR = os.path.join(BASE_OUTPUT_DIR_ARTIST, CLEAN_TARGET_ARTIST_NAME)\n",
    "MODEL_DIR_GENERAL = os.path.join(MODEL_DIR_BASE_GENERAL)\n",
    "MODEL_DIR_ARTIST_FINE_TUNED = os.path.join(MODEL_DIR_BASE_ARTIST_FINE_TUNED, CLEAN_TARGET_ARTIST_NAME)\n",
    "os.makedirs(MODEL_DIR_GENERAL, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR_ARTIST_FINE_TUNED, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- Definições de Classes PyTorch ---\n",
    "class LyricsDataset(Dataset):\n",
    "    \"\"\"Carrega dados tokenizados .npy de forma eficiente para o PyTorch.\"\"\"\n",
    "    def __init__(self, x_tokens_path, y_tokens_path):\n",
    "        self.x_data = np.load(x_tokens_path, mmap_mode='r')\n",
    "        self.y_data = np.load(y_tokens_path, mmap_mode='r')\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    def __getitem__(self, idx):\n",
    "        x_sample = torch.tensor(self.x_data[idx], dtype=torch.long)\n",
    "        y_sample = torch.tensor(self.y_data[idx], dtype=torch.long)\n",
    "        return x_sample, y_sample\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    \"\"\"Arquitetura do modelo LSTM para geração de caracteres.\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate, dropout_strategy=\"none\"):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_strategy = dropout_strategy\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        lstm_dropout_param = dropout_rate if dropout_strategy == \"between_lstm\" and n_layers > 1 else 0.0\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            n_layers,\n",
    "                            dropout=lstm_dropout_param,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        final_lstm_output = output[:, -1, :]\n",
    "        if self.dropout_strategy == \"before_fc\":\n",
    "            final_lstm_output = self.dropout_layer(final_lstm_output)\n",
    "        prediction = self.fc(final_lstm_output)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Carrega o corpus geral pré-processado e o tokenizador correspondente. Em\n",
    "seguida, executa a primeira fase do treinamento: o pré-treinamento do modelo\n",
    "CharLSTM no grande corpus de letras de múltiplos artistas. O modelo com a menor\n",
    "perda de validação e seu histórico de treinamento são salvos no disco para a\n",
    "subsequente fase de fine-tuning.\n",
    "\"\"\"\n",
    "try:\n",
    "    tokenizer_general = load_object(os.path.join(GENERAL_DATA_DIR, TOKENIZER_SUBDIR, 'char_tokenizer_general.pkl'))\n",
    "    total_chars_general = load_object(os.path.join(GENERAL_DATA_DIR, TOKENIZER_SUBDIR, 'vocab_size_general_char.pkl'))\n",
    "    print(f\"Vocabulário de caracteres geral carregado. Tamanho: {total_chars_general}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Erro: Arquivos do tokenizer geral não encontrados. Detalhes: {e}.\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\n--- Iniciando Fase 1: Treinamento do Modelo Geral ---\")\n",
    "X_train_general_path = os.path.join(GENERAL_DATA_DIR, DATA_SUBDIR, 'X_train_general_char.npy')\n",
    "y_train_general_path = os.path.join(GENERAL_DATA_DIR, DATA_SUBDIR, 'y_train_general_char.npy')\n",
    "X_val_general_path = os.path.join(GENERAL_DATA_DIR, DATA_SUBDIR, 'X_val_general_char.npy')\n",
    "y_val_general_path = os.path.join(GENERAL_DATA_DIR, DATA_SUBDIR, 'y_val_general_char.npy')\n",
    "\n",
    "if not all(os.path.exists(p) for p in [X_train_general_path, y_train_general_path, X_val_general_path, y_val_general_path]):\n",
    "    print(f\"Dados gerais de treino/validação não encontrados em {GENERAL_DATA_DIR}/{DATA_SUBDIR}. Execute o pré-processamento.\")\n",
    "    exit()\n",
    "\n",
    "train_dataset_general = LyricsDataset(X_train_general_path, y_train_general_path)\n",
    "val_dataset_general = LyricsDataset(X_val_general_path, y_val_general_path)\n",
    "train_loader_general = DataLoader(train_dataset_general, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader_general = DataLoader(val_dataset_general, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "print(f\"Dataset de treino geral: {len(train_dataset_general)} amostras.\")\n",
    "print(f\"Dataset de validação geral: {len(val_dataset_general)} amostras.\")\n",
    "\n",
    "CURRENT_DROPOUT_STRATEGY_GENERAL = \"none\"\n",
    "\n",
    "model_general = CharLSTM(\n",
    "    vocab_size=total_chars_general,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    dropout_strategy=CURRENT_DROPOUT_STRATEGY_GENERAL\n",
    ").to(device)\n",
    "\n",
    "print(\"\\nModelo Geral (Base):\", model_general, sep='\\n')\n",
    "criterion_general = nn.CrossEntropyLoss()\n",
    "optimizer_general = optim.Adam(model_general.parameters(), lr=LEARNING_RATE_GENERAL)\n",
    "\n",
    "best_val_loss_general = float('inf')\n",
    "model_general_save_filepath = os.path.join(MODEL_DIR_GENERAL, f\"model_weights_general_strategy_{CURRENT_DROPOUT_STRATEGY_GENERAL}_best.pt\")\n",
    "history_general = {'train_loss': [], 'val_loss': [], 'train_accuracy': [], 'val_accuracy': [], 'dropout_strategy': CURRENT_DROPOUT_STRATEGY_GENERAL}\n",
    "\n",
    "print(f\"Iniciando treinamento geral com estratégia de dropout: '{CURRENT_DROPOUT_STRATEGY_GENERAL}'...\")\n",
    "for epoch in range(EPOCHS_GENERAL):\n",
    "    model_general.train()\n",
    "    train_loss, correct_predictions, total_predictions = 0.0, 0, 0\n",
    "    for inputs, labels in tqdm(train_loader_general, desc=f\"Epoch {epoch+1}/{EPOCHS_GENERAL} [Train General]\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer_general.zero_grad()\n",
    "        outputs = model_general(inputs)\n",
    "        loss = criterion_general(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_general.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_predictions += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "    \n",
    "    history_general['train_loss'].append(train_loss / len(train_dataset_general))\n",
    "    history_general['train_accuracy'].append(correct_predictions / total_predictions)\n",
    "\n",
    "    model_general.eval()\n",
    "    val_loss, correct_predictions, total_predictions = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader_general, desc=f\"Epoch {epoch+1}/{EPOCHS_GENERAL} [Validation General]\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_general(inputs)\n",
    "            loss = criterion_general(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    current_val_loss = val_loss / len(val_dataset_general)\n",
    "    history_general['val_loss'].append(current_val_loss)\n",
    "    history_general['val_accuracy'].append(correct_predictions / total_predictions)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS_GENERAL}: Train Loss: {history_general['train_loss'][-1]:.4f}, Val Loss: {current_val_loss:.4f}\")\n",
    "\n",
    "    if current_val_loss < best_val_loss_general:\n",
    "        best_val_loss_general = current_val_loss\n",
    "        torch.save(model_general.state_dict(), model_general_save_filepath)\n",
    "        print(f\"Modelo Geral salvo: Melhor val_loss em {best_val_loss_general:.4f}\")\n",
    "\n",
    "history_general_filepath = os.path.join(MODEL_DIR_GENERAL, f\"training_history_general_strategy_{CURRENT_DROPOUT_STRATEGY_GENERAL}.pkl\")\n",
    "with open(history_general_filepath, 'wb') as f:\n",
    "    pickle.dump(history_general, f)\n",
    "\n",
    "print(f\"\\nTreinamento geral concluído. Melhor modelo salvo em: {model_general_save_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e3014c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Objeto carregado de: processed_data_general_char_split\\tokenizers\\char_tokenizer_general.pkl\n",
      "Objeto carregado de: processed_data_general_char_split\\tokenizers\\vocab_size_general_char.pkl\n",
      "Vocabulário de caracteres GERAL carregado. Tamanho do vocabulário: 36\n",
      "\n",
      "--- FASE 1: TREINAMENTO DO MODELO GERAL ---\n",
      "Dataset de treino geral com 7797308 amostras, 60917 batches.\n",
      "Dataset de validação geral com 1020540 amostras, 7973 batches.\n",
      "\n",
      "Modelo Geral (Base):\n",
      "CharLSTM(\n",
      "  (embedding): Embedding(36, 256)\n",
      "  (lstm): LSTM(256, 256, num_layers=4, batch_first=True, bidirectional=True)\n",
      "  (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=36, bias=True)\n",
      ")\n",
      "Iniciando treinamento GERAL com estratégia de dropout: 'none'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [Train General]: 100%|██████████| 60917/60917 [1:53:21<00:00,  8.96it/s]\n",
      "Epoch 1/3 [Validation General]: 100%|██████████| 7973/7973 [04:56<00:00, 26.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: Train Loss: 1.2711, Train Acc: 0.6024, Val Loss: 1.2332, Val Acc: 0.6149\n",
      "Modelo Geral salvo: Melhor val_loss em 1.2332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 [Train General]: 100%|██████████| 60917/60917 [1:51:37<00:00,  9.10it/s]\n",
      "Epoch 2/3 [Validation General]: 100%|██████████| 7973/7973 [04:57<00:00, 26.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: Train Loss: 1.1536, Train Acc: 0.6386, Val Loss: 1.2172, Val Acc: 0.6209\n",
      "Modelo Geral salvo: Melhor val_loss em 1.2172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 [Train General]: 100%|██████████| 60917/60917 [1:55:54<00:00,  8.76it/s]\n",
      "Epoch 3/3 [Validation General]: 100%|██████████| 7973/7973 [05:00<00:00, 26.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: Train Loss: 1.1402, Train Acc: 0.6430, Val Loss: 1.2172, Val Acc: 0.6205\n",
      "Modelo Geral salvo: Melhor val_loss em 1.2172\n",
      "\n",
      "Histórico de treinamento GERAL salvo em: models_general_char_split\\training_history_general_strategy_none.pkl\n",
      "\n",
      "Treinamento GERAL concluído. Melhor modelo salvo em: models_general_char_split\\model_weights_general_strategy_none_best.pt\n",
      "\n",
      "--- FASE 2: FINE-TUNING PARA O ARTISTA ALVO ---\n",
      "Dataset de treino do artista com 368522 amostras, 2880 batches.\n",
      "Dataset de validação do artista com 39919 amostras, 312 batches.\n",
      "Modelo pré-treinado carregado de: models_general_char_split\\model_weights_general_strategy_none_best.pt\n",
      "\n",
      "Modelo para Fine-tuning (inicializado com pesos gerais):\n",
      "CharLSTM(\n",
      "  (embedding): Embedding(36, 256)\n",
      "  (lstm): LSTM(256, 256, num_layers=4, batch_first=True, bidirectional=True)\n",
      "  (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=36, bias=True)\n",
      ")\n",
      "Iniciando FINE-TUNING para 'ArianaGrande' com estratégia de dropout: 'none'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Fine-tune]: 100%|██████████| 2880/2880 [04:54<00:00,  9.78it/s]\n",
      "Epoch 1/10 [Validation Fine-tune]: 100%|██████████| 312/312 [00:11<00:00, 27.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss: 1.0320, Train Acc: 0.6822, Val Loss: 0.9359, Val Acc: 0.7104\n",
      "Modelo Fine-tuned salvo: Melhor val_loss em 0.9359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Fine-tune]: 100%|██████████| 2880/2880 [04:48<00:00,  9.98it/s]\n",
      "Epoch 2/10 [Validation Fine-tune]: 100%|██████████| 312/312 [00:11<00:00, 27.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: Train Loss: 0.8600, Train Acc: 0.7390, Val Loss: 0.8641, Val Acc: 0.7358\n",
      "Modelo Fine-tuned salvo: Melhor val_loss em 0.8641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Fine-tune]: 100%|██████████| 2880/2880 [04:49<00:00,  9.94it/s]\n",
      "Epoch 3/10 [Validation Fine-tune]: 100%|██████████| 312/312 [00:11<00:00, 27.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: Train Loss: 0.7524, Train Acc: 0.7757, Val Loss: 0.8114, Val Acc: 0.7544\n",
      "Modelo Fine-tuned salvo: Melhor val_loss em 0.8114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Fine-tune]: 100%|██████████| 2880/2880 [04:50<00:00,  9.92it/s]\n",
      "Epoch 4/10 [Validation Fine-tune]: 100%|██████████| 312/312 [00:11<00:00, 27.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: Train Loss: 0.6673, Train Acc: 0.8044, Val Loss: 0.7686, Val Acc: 0.7715\n",
      "Modelo Fine-tuned salvo: Melhor val_loss em 0.7686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Fine-tune]: 100%|██████████| 2880/2880 [04:50<00:00,  9.90it/s]\n",
      "Epoch 5/10 [Validation Fine-tune]: 100%|██████████| 312/312 [00:11<00:00, 27.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: Train Loss: 0.5960, Train Acc: 0.8282, Val Loss: 0.7330, Val Acc: 0.7843\n",
      "Modelo Fine-tuned salvo: Melhor val_loss em 0.7330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Fine-tune]: 100%|██████████| 2880/2880 [04:49<00:00,  9.94it/s]\n",
      "Epoch 6/10 [Validation Fine-tune]: 100%|██████████| 312/312 [00:11<00:00, 28.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: Train Loss: 0.5340, Train Acc: 0.8490, Val Loss: 0.7037, Val Acc: 0.7950\n",
      "Modelo Fine-tuned salvo: Melhor val_loss em 0.7037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Fine-tune]: 100%|██████████| 2880/2880 [04:49<00:00,  9.95it/s]\n",
      "Epoch 7/10 [Validation Fine-tune]: 100%|██████████| 312/312 [00:11<00:00, 27.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: Train Loss: 0.4791, Train Acc: 0.8674, Val Loss: 0.6797, Val Acc: 0.8042\n",
      "Modelo Fine-tuned salvo: Melhor val_loss em 0.6797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Fine-tune]: 100%|██████████| 2880/2880 [04:50<00:00,  9.91it/s]\n",
      "Epoch 8/10 [Validation Fine-tune]: 100%|██████████| 312/312 [00:11<00:00, 27.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: Train Loss: 0.4296, Train Acc: 0.8831, Val Loss: 0.6602, Val Acc: 0.8110\n",
      "Modelo Fine-tuned salvo: Melhor val_loss em 0.6602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Fine-tune]: 100%|██████████| 2880/2880 [04:49<00:00,  9.94it/s]\n",
      "Epoch 9/10 [Validation Fine-tune]: 100%|██████████| 312/312 [00:11<00:00, 27.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: Train Loss: 0.3848, Train Acc: 0.8974, Val Loss: 0.6410, Val Acc: 0.8199\n",
      "Modelo Fine-tuned salvo: Melhor val_loss em 0.6410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Fine-tune]: 100%|██████████| 2880/2880 [04:49<00:00,  9.93it/s]\n",
      "Epoch 10/10 [Validation Fine-tune]: 100%|██████████| 312/312 [00:11<00:00, 27.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: Train Loss: 0.3443, Train Acc: 0.9101, Val Loss: 0.6273, Val Acc: 0.8264\n",
      "Modelo Fine-tuned salvo: Melhor val_loss em 0.6273\n",
      "\n",
      "Histórico de fine-tuning salvo em: models_by_artist_char_split_finetuned\\arianagrande\\training_history_arianagrande_finetuned_strategy_none.pkl\n",
      "\n",
      "Fine-tuning para 'ArianaGrande' com estratégia 'none' concluído.\n",
      "O melhor modelo fine-tuned foi salvo em: models_by_artist_char_split_finetuned\\arianagrande\\model_weights_arianagrande_finetuned_strategy_none_best.pt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Executa a segunda fase do treinamento: o fine-tuning. Carrega os datasets\n",
    "exclusivos do artista alvo, inicializa uma nova arquitetura de modelo e\n",
    "carrega os pesos do modelo geral pré-treinado na etapa anterior. Em seguida,\n",
    "continua o treinamento (fine-tuning), geralmente com uma taxa de aprendizado\n",
    "menor, usando apenas os dados do artista. Salva o modelo especializado\n",
    "resultante e seu histórico de treinamento.\n",
    "\"\"\"\n",
    "print(\"\\n--- Iniciando Fase 2: Fine-tuning para o Artista Alvo ---\")\n",
    "\n",
    "X_train_artist_path = os.path.join(ARTIST_DATA_DIR, DATA_SUBDIR, f'X_train_{CLEAN_TARGET_ARTIST_NAME}_char.npy')\n",
    "y_train_artist_path = os.path.join(ARTIST_DATA_DIR, DATA_SUBDIR, f'y_train_{CLEAN_TARGET_ARTIST_NAME}_char.npy')\n",
    "X_val_artist_path = os.path.join(ARTIST_DATA_DIR, DATA_SUBDIR, f'X_val_{CLEAN_TARGET_ARTIST_NAME}_char.npy')\n",
    "y_val_artist_path = os.path.join(ARTIST_DATA_DIR, DATA_SUBDIR, f'y_val_{CLEAN_TARGET_ARTIST_NAME}_char.npy')\n",
    "\n",
    "if not all(os.path.exists(p) for p in [X_train_artist_path, y_train_artist_path, X_val_artist_path, y_val_artist_path]):\n",
    "    print(f\"Dados do artista alvo não encontrados em {ARTIST_DATA_DIR}/{DATA_SUBDIR}. Execute o pré-processamento.\")\n",
    "    exit()\n",
    "\n",
    "train_dataset_artist = LyricsDataset(X_train_artist_path, y_train_artist_path)\n",
    "val_dataset_artist = LyricsDataset(X_val_artist_path, y_val_artist_path)\n",
    "train_loader_artist = DataLoader(train_dataset_artist, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader_artist = DataLoader(val_dataset_artist, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "print(f\"Dataset de treino do artista: {len(train_dataset_artist)} amostras.\")\n",
    "print(f\"Dataset de validação do artista: {len(val_dataset_artist)} amostras.\")\n",
    "\n",
    "CURRENT_DROPOUT_STRATEGY_FINE_TUNE = \"none\"\n",
    "\n",
    "model_fine_tune = CharLSTM(\n",
    "    vocab_size=total_chars_general,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    dropout_strategy=CURRENT_DROPOUT_STRATEGY_FINE_TUNE\n",
    ").to(device)\n",
    "\n",
    "model_general_save_filepath_for_fine_tune = os.path.join(MODEL_DIR_GENERAL, f\"model_weights_general_strategy_{CURRENT_DROPOUT_STRATEGY_FINE_TUNE}_best.pt\")\n",
    "try:\n",
    "    model_fine_tune.load_state_dict(torch.load(model_general_save_filepath_for_fine_tune, map_location=device))\n",
    "    print(f\"Modelo pré-treinado carregado de: {model_general_save_filepath_for_fine_tune}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Modelo geral pré-treinado com estratégia '{CURRENT_DROPOUT_STRATEGY_FINE_TUNE}' não encontrado. Treine o modelo geral primeiro.\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\nModelo para Fine-tuning (inicializado com pesos gerais):\", model_fine_tune, sep='\\n')\n",
    "criterion_fine_tune = nn.CrossEntropyLoss()\n",
    "optimizer_fine_tune = optim.Adam(model_fine_tune.parameters(), lr=LEARNING_RATE_FINE_TUNE)\n",
    "\n",
    "best_val_loss_fine_tune = float('inf')\n",
    "model_fine_tune_save_filepath = os.path.join(MODEL_DIR_ARTIST_FINE_TUNED, f\"model_weights_{CLEAN_TARGET_ARTIST_NAME}_finetuned_strategy_{CURRENT_DROPOUT_STRATEGY_FINE_TUNE}_best.pt\")\n",
    "history_fine_tune = {'train_loss': [], 'val_loss': [], 'train_accuracy': [], 'val_accuracy': [], 'dropout_strategy': CURRENT_DROPOUT_STRATEGY_FINE_TUNE}\n",
    "\n",
    "print(f\"Iniciando fine-tuning para '{TARGET_ARTIST}' com estratégia de dropout: '{CURRENT_DROPOUT_STRATEGY_FINE_TUNE}'...\")\n",
    "for epoch in range(EPOCHS_FINE_TUNE):\n",
    "    model_fine_tune.train()\n",
    "    train_loss, correct_predictions, total_predictions = 0.0, 0, 0\n",
    "    for inputs, labels in tqdm(train_loader_artist, desc=f\"Epoch {epoch+1}/{EPOCHS_FINE_TUNE} [Fine-tune]\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer_fine_tune.zero_grad()\n",
    "        outputs = model_fine_tune(inputs)\n",
    "        loss = criterion_fine_tune(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_fine_tune.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_predictions += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    history_fine_tune['train_loss'].append(train_loss / len(train_dataset_artist))\n",
    "    history_fine_tune['train_accuracy'].append(correct_predictions / total_predictions)\n",
    "    \n",
    "    model_fine_tune.eval()\n",
    "    val_loss, correct_predictions, total_predictions = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader_artist, desc=f\"Epoch {epoch+1}/{EPOCHS_FINE_TUNE} [Validation Fine-tune]\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_fine_tune(inputs)\n",
    "            loss = criterion_fine_tune(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    current_val_loss = val_loss / len(val_dataset_artist)\n",
    "    history_fine_tune['val_loss'].append(current_val_loss)\n",
    "    history_fine_tune['val_accuracy'].append(correct_predictions / total_predictions)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS_FINE_TUNE}: Train Loss: {history_fine_tune['train_loss'][-1]:.4f}, Val Loss: {current_val_loss:.4f}\")\n",
    "\n",
    "    if current_val_loss < best_val_loss_fine_tune:\n",
    "        best_val_loss_fine_tune = current_val_loss\n",
    "        torch.save(model_fine_tune.state_dict(), model_fine_tune_save_filepath)\n",
    "        print(f\"Modelo Fine-tuned salvo: Melhor val_loss em {best_val_loss_fine_tune:.4f}\")\n",
    "\n",
    "history_fine_tune_filepath = os.path.join(MODEL_DIR_ARTIST_FINE_TUNED, f\"training_history_{CLEAN_TARGET_ARTIST_NAME}_finetuned_strategy_{CURRENT_DROPOUT_STRATEGY_FINE_TUNE}.pkl\")\n",
    "with open(history_fine_tune_filepath, 'wb') as f:\n",
    "    pickle.dump(history_fine_tune, f)\n",
    "\n",
    "print(f\"\\nHistórico de fine-tuning salvo em: {history_fine_tune_filepath}\")\n",
    "print(f\"Fine-tuning para '{TARGET_ARTIST}' concluído. Melhor modelo salvo em: {model_fine_tune_save_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c26664",
   "metadata": {},
   "source": [
    "## Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac99bba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'wordnet' NLTK package...\n",
      "Using device: cuda\n",
      "Object loaded from: processed_data_general_char_split\\tokenizers\\char_tokenizer_general.pkl\n",
      "Object loaded from: processed_data_general_char_split\\tokenizers\\vocab_size_general_char.pkl\n",
      "General character vocabulary loaded. Vocabulary size: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model 'en_core_web_sm' loaded.\n",
      "Size of existing word vocabulary in general corpus (for metrics): 40909\n",
      "Total 29 songs in the artist's test set for full lyric generation evaluation.\n",
      "\n",
      "Attempting to load fine-tuned model from: models_by_artist_char_split_finetuned\\arianagrande\\arianagrande_char_lstm_fine_tuned_model.pth\n",
      "Error: Fine-tuned model not found at models_by_artist_char_split_finetuned\\arianagrande\\arianagrande_char_lstm_fine_tuned_model.pth. Please ensure the fine-tuning script has been run and the model saved correctly.\n",
      "\n",
      "--- Avaliação de Geração de Músicas (Música a Música) para 'ArianaGrande' (Modelo Fine-tuned com Dropout: none) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating words: 100%|██████████| 3/3 [00:02<00:00,  1.25it/s][00:00<?, ?it/s]\n",
      "Generating words: 100%|██████████| 673/673 [06:56<00:00,  1.62it/s]02<01:07,  2.41s/it]\n",
      "Generating words: 100%|██████████| 775/775 [07:05<00:00,  1.82it/s]58<1:50:42, 246.03s/it]\n",
      "Generating words: 100%|██████████| 519/519 [05:13<00:00,  1.65it/s]04<2:22:06, 327.93s/it]\n",
      "Generating words: 514it [06:02,  1.42it/s]14%|█▍        | 4/29 [19:18<2:14:17, 322.31s/it]\n",
      "Generating words: 100%|██████████| 402/402 [05:24<00:00,  1.24it/s]20<2:14:46, 336.95s/it]\n",
      "Generating words: 100%|██████████| 372/372 [05:05<00:00,  1.22it/s]45<2:07:30, 332.62s/it]\n",
      "Generating words: 100%|██████████| 381/381 [04:49<00:00,  1.32it/s]50<1:58:41, 323.69s/it]\n",
      "Generating words: 100%|██████████| 14/14 [00:09<00:00,  1.55it/s]0:40<1:49:29, 312.81s/it]\n",
      "Generating words: 100%|██████████| 316/316 [03:38<00:00,  1.45it/s]49<1:12:37, 217.86s/it]\n",
      "Generating words: 100%|██████████| 526/526 [05:20<00:00,  1.64it/s]:27<1:09:03, 218.09s/it]\n",
      "Generating words: 100%|██████████| 13/13 [00:04<00:00,  2.90it/s]49:47<1:14:47, 249.32s/it]\n",
      "Generating words: 100%|██████████| 337/337 [03:44<00:00,  1.50it/s]:52<49:32, 174.84s/it]  \n",
      "Generating words: 90it [00:55,  1.63it/s] 45%|████▍     | 13/29 [53:37<50:40, 190.00s/it]\n",
      "Generating words: 100%|██████████| 336/336 [02:46<00:00,  2.02it/s]:32<37:19, 149.27s/it]\n",
      "Generating words: 100%|██████████| 23/23 [00:12<00:00,  1.80it/s]57:18<36:01, 154.38s/it]\n",
      "Generating words: 100%|██████████| 494/494 [03:30<00:00,  2.35it/s]:31<24:12, 111.75s/it]\n",
      "Generating words: 444it [04:33,  1.62it/s]59%|█████▊    | 17/29 [1:01:01<28:16, 141.35s/it]\n",
      "Generating words: 100%|██████████| 45/45 [00:17<00:00,  2.57it/s]1:05:35<33:12, 181.14s/it]\n",
      "Generating words: 270it [02:21,  1.90it/s]66%|██████▌   | 19/29 [1:05:52<21:59, 132.00s/it]\n",
      "Generating words: 385it [04:42,  1.36it/s]69%|██████▉   | 20/29 [1:08:14<20:14, 135.00s/it]\n",
      "Generating words: 100%|██████████| 477/477 [05:10<00:00,  1.54it/s]12:57<23:55, 179.41s/it]\n",
      "Generating words: 100%|██████████| 154/154 [01:40<00:00,  1.53it/s]18:08<25:31, 218.81s/it]\n",
      "Generating words: 100%|██████████| 463/463 [04:35<00:00,  1.68it/s]19:49<18:20, 183.43s/it]\n",
      "Generating words: 360it [03:42,  1.62it/s]83%|████████▎ | 24/29 [1:24:25<17:35, 211.16s/it]\n",
      "Generating words: 100%|██████████| 261/261 [03:12<00:00,  1.36it/s]28:08<14:18, 214.66s/it]\n",
      "Generating words: 100%|██████████| 153/153 [01:27<00:00,  1.74it/s]31:20<10:23, 207.95s/it]\n",
      "Generating words: 100%|██████████| 567/567 [06:14<00:00,  1.52it/s]32:48<05:43, 171.93s/it]\n",
      "Generating words: 100%|██████████| 19/19 [00:12<00:00,  1.47it/s]1:39:02<03:52, 232.65s/it]\n",
      "Generating & Evaluating (Dropout: none): 100%|██████████| 29/29 [1:39:15<00:00, 205.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "--- MÉDIAS DAS MÉTRICAS DE GERAÇÃO (Dropout: none) ---\n",
      "--- Letras Geradas (Média) ---\n",
      "Total Words (Avg): 344.52\n",
      "Unique Words (Avg): 308.24\n",
      "TTR (Distinct-1 Ratio) (Avg): 0.8900\n",
      "Repetition Rate (1-Distinct-1) (Avg): 0.1100\n",
      "Distinct-2 Ratio (Bigrams) (Avg): 0.9826\n",
      "Repetition Rate (1-Distinct-2) (Bigrams) (Avg): 0.0174\n",
      "Distinct-3 Ratio (Trigrams) (Avg): 0.9909\n",
      "Repetition Rate (1-Distinct-3) (Trigrams) (Avg): 0.0091\n",
      "Generated Unique Words Existing in Corpus (Avg): 33.07\n",
      "Existence Rate (Generated Unique Words) (Avg): 0.18%\n",
      "-----------------------------------------------------\n",
      "\n",
      "--- Letras Originais do Teste (Média) ---\n",
      "Total Words (Avg): 342.93\n",
      "Unique Words (Avg): 91.79\n",
      "TTR (Distinct-1 Ratio) (Avg): 0.3644\n",
      "Repetition Rate (1-Distinct-1) (Avg): 0.6356\n",
      "Distinct-2 Ratio (Bigrams) (Avg): 0.5176\n",
      "Repetition Rate (1-Distinct-2) (Bigrams) (Avg): 0.4824\n",
      "Distinct-3 Ratio (Trigrams) (Avg): 0.5837\n",
      "Repetition Rate (1-Distinct-3) (Trigrams) (Avg): 0.4163\n",
      "Actual Unique Words Existing in Corpus (Avg): 91.79\n",
      "Existence Rate (Actual Unique Words) (Avg): 1.00%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Executa um fluxo de avaliação completo e autônomo para um modelo de geração de\n",
    "texto fine-tuned. Este script define todas as configurações, funções e classes\n",
    "necessárias, prepara os dados de avaliação (incluindo o vocabulário de referência\n",
    "de todo o corpus e as letras de teste do artista alvo), carrega o modelo\n",
    "fine-tuned treinado, e então executa uma avaliação música a música. Para cada\n",
    "música no conjunto de teste, ele gera uma nova letra de comprimento equivalente e\n",
    "calcula um conjunto de métricas de qualidade de texto (e.g., Distinct-N, TTR,\n",
    "taxa de repetição). Finalmente, agrega e exibe as métricas médias, comparando o\n",
    "desempenho do texto gerado com o das letras originais.\n",
    "\"\"\"\n",
    "\n",
    "def get_word_ngrams(text, n):\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    if len(words) < n: return []\n",
    "    return list(nltk.ngrams(words, n))\n",
    "\n",
    "def calculate_distinct_n_ratio(text, n):\n",
    "    ngrams = get_word_ngrams(text, n)\n",
    "    if not ngrams: return 0.0\n",
    "    return len(set(ngrams)) / len(ngrams)\n",
    "\n",
    "def calculate_text_quality_metrics_updated(text_to_analyze, reference_words_set):\n",
    "    words = nltk.word_tokenize(text_to_analyze.lower())\n",
    "    metrics = {\n",
    "        'Total Words': len(words), 'Unique Words': 0, 'TTR (Distinct-1 Ratio)': 0.0,\n",
    "        'Repetition Rate (1-Distinct-1)': 0.0, 'Distinct-2 Ratio (Bigrams)': 0.0,\n",
    "        'Repetition Rate (1-Distinct-2) (Bigrams)': 0.0, 'Distinct-3 Ratio (Trigrams)': 0.0,\n",
    "        'Repetition Rate (1-Distinct-3) (Trigrams)': 0.0, 'Generated Unique Words Existing in Corpus': 0,\n",
    "        'Existence Rate (Generated Unique Words)': 0.0\n",
    "    }\n",
    "    if not words: return metrics\n",
    "    unique_words = set(words)\n",
    "    metrics['Unique Words'] = len(unique_words)\n",
    "    metrics['TTR (Distinct-1 Ratio)'] = calculate_distinct_n_ratio(text_to_analyze, 1)\n",
    "    metrics['Repetition Rate (1-Distinct-1)'] = 1 - metrics['TTR (Distinct-1 Ratio)']\n",
    "    metrics['Distinct-2 Ratio (Bigrams)'] = calculate_distinct_n_ratio(text_to_analyze, 2)\n",
    "    metrics['Repetition Rate (1-Distinct-2) (Bigrams)'] = 1 - metrics['Distinct-2 Ratio (Bigrams)']\n",
    "    metrics['Distinct-3 Ratio (Trigrams)'] = calculate_distinct_n_ratio(text_to_analyze, 3)\n",
    "    metrics['Repetition Rate (1-Distinct-3) (Trigrams)'] = 1 - metrics['Distinct-3 Ratio (Trigrams)']\n",
    "    existing_words_in_generated = sum(1 for word in unique_words if word in reference_words_set)\n",
    "    metrics['Generated Unique Words Existing in Corpus'] = existing_words_in_generated\n",
    "    metrics['Existence Rate (Generated Unique Words)'] = existing_words_in_generated / len(unique_words) if unique_words else 0.0\n",
    "    return metrics\n",
    "\n",
    "def generate_text_by_word_count(model, tokenizer, seed_text, target_word_count, temperature=0.8, sequence_length=SEQUENCE_LENGTH, device=device, max_chars_per_word=25):\n",
    "    model.eval()\n",
    "    processed_seed_text = clean_text(seed_text)\n",
    "    if len(processed_seed_text) < sequence_length:\n",
    "        processed_seed_text = ' ' * (sequence_length - len(processed_seed_text)) + processed_seed_text\n",
    "    else:\n",
    "        processed_seed_text = processed_seed_text[-sequence_length:]\n",
    "    \n",
    "    generated_text = processed_seed_text\n",
    "    current_word_count = len(nltk.word_tokenize(processed_seed_text))\n",
    "    pbar = tqdm(total=max(0, target_word_count - current_word_count), desc=\"Generating words\")\n",
    "    max_chars_to_generate = target_word_count * max_chars_per_word * 2\n",
    "    chars_generated = 0\n",
    "    \n",
    "    while current_word_count < target_word_count and chars_generated < max_chars_to_generate:\n",
    "        current_sequence_str = generated_text[-sequence_length:]\n",
    "        tokenized_input = tokenizer.texts_to_sequences(current_sequence_str)\n",
    "        if not tokenized_input or all(t == 0 for t in tokenized_input): break\n",
    "        \n",
    "        input_tensor = torch.tensor(tokenized_input, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor).squeeze(0)\n",
    "        \n",
    "        if temperature <= 0:\n",
    "            next_char_id = torch.argmax(predictions, dim=-1).item()\n",
    "        else:\n",
    "            probabilities = torch.softmax(predictions / temperature, dim=-1)\n",
    "            if probabilities.sum() == 0 or torch.isnan(probabilities).any():\n",
    "                next_char_id = random.randint(1, tokenizer.vocab_size - 1)\n",
    "            else:\n",
    "                next_char_id = torch.multinomial(probabilities, 1).item()\n",
    "        \n",
    "        next_char = tokenizer.int_to_char.get(next_char_id, '')\n",
    "        generated_text += next_char\n",
    "        chars_generated += 1\n",
    "        \n",
    "        if chars_generated % 50 == 0 or next_char.isspace() or next_char in '!?,.':\n",
    "            new_word_count = len(nltk.word_tokenize(generated_text))\n",
    "            if new_word_count > current_word_count:\n",
    "                pbar.update(new_word_count - current_word_count)\n",
    "                current_word_count = new_word_count\n",
    "    pbar.close()\n",
    "    return generated_text\n",
    "\n",
    "# --- Preparação dos Dados e Recursos de Avaliação ---\n",
    "try:\n",
    "    tokenizer_general = load_object(os.path.join(BASE_OUTPUT_DIR_GENERAL, TOKENIZER_SUBDIR, 'char_tokenizer_general.pkl'))\n",
    "    total_chars_general = load_object(os.path.join(BASE_OUTPUT_DIR_GENERAL, TOKENIZER_SUBDIR, 'vocab_size_general_char.pkl'))\n",
    "    print(f\"General character vocabulary loaded. Size: {total_chars_general}\")\n",
    "    \n",
    "    existing_words_in_corpus = set()\n",
    "    for filename in os.listdir(CSV_FOLDER_PATH):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            df_temp = pd.read_csv(os.path.join(CSV_FOLDER_PATH, filename), usecols=['Lyric'])\n",
    "            for lyric_val in df_temp['Lyric']:\n",
    "                cleaned_lyric = clean_text(lyric_val)\n",
    "                if cleaned_lyric and PLACEHOLDER_PHRASE not in cleaned_lyric:\n",
    "                    existing_words_in_corpus.update(nltk.word_tokenize(cleaned_lyric))\n",
    "    print(f\"Size of existing word vocabulary in general corpus: {len(existing_words_in_corpus)}\")\n",
    "\n",
    "    ARTIST_LYRICS_CLEAN_ALL_FOR_PROMPTS = []\n",
    "    df_target_artist_full = pd.read_csv(CSV_FILE_PATH_TARGET_ARTIST, usecols=['Lyric'])\n",
    "    for lyric_val in df_target_artist_full['Lyric']:\n",
    "        cleaned_lyric = clean_text(lyric_val)\n",
    "        if cleaned_lyric and PLACEHOLDER_PHRASE not in cleaned_lyric:\n",
    "            ARTIST_LYRICS_CLEAN_ALL_FOR_PROMPTS.append(cleaned_lyric)\n",
    "    \n",
    "    random.seed(RANDOM_SEED)\n",
    "    random.shuffle(ARTIST_LYRICS_CLEAN_ALL_FOR_PROMPTS)\n",
    "    num_test_artist_final = int(len(ARTIST_LYRICS_CLEAN_ALL_FOR_PROMPTS) * TEST_SPLIT)\n",
    "    TEST_LYRICS_ARTIST_FINAL_SONGS = ARTIST_LYRICS_CLEAN_ALL_FOR_PROMPTS[:num_test_artist_final]\n",
    "    print(f\"Total {len(TEST_LYRICS_ARTIST_FINAL_SONGS)} songs in artist's test set for evaluation.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing data: {e}\")\n",
    "    TEST_LYRICS_ARTIST_FINAL_SONGS = []\n",
    "    existing_words_in_corpus = set()\n",
    "    exit()\n",
    "\n",
    "# --- Carregamento do Modelo Fine-Tuned ---\n",
    "MODEL_DIR_ARTIST_FINE_TUNED_SPECIFIC = os.path.join(MODEL_DIR_BASE_ARTIST_FINE_TUNED, CLEAN_TARGET_ARTIST_NAME)\n",
    "MODEL_PATH = os.path.join(MODEL_DIR_ARTIST_FINE_TUNED_SPECIFIC, f'model_weights_{CLEAN_TARGET_ARTIST_NAME}_finetuned_strategy_none_best.pt')\n",
    "print(f\"\\nAttempting to load fine-tuned model from: {MODEL_PATH}\")\n",
    "try:\n",
    "    dropout_strategy_for_model = 'none'\n",
    "    model_fine_tuned = CharLSTM(\n",
    "        vocab_size=total_chars_general, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM,\n",
    "        n_layers=N_LAYERS, dropout_rate=DROPOUT_RATE, dropout_strategy=dropout_strategy_for_model\n",
    "    ).to(device)\n",
    "    model_fine_tuned.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    model_fine_tuned.eval()\n",
    "    print(\"Fine-tuned model loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Fine-tuned model not found at {MODEL_PATH}.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the fine-tuned model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Execução da Avaliação Música a Música ---\n",
    "dropout_strategy_for_display = dropout_strategy_for_model\n",
    "print(f\"\\n--- Avaliação de Geração (Modelo Fine-tuned com Dropout: {dropout_strategy_for_display}) ---\")\n",
    "\n",
    "all_generated_lyrics_metrics = []\n",
    "all_actual_lyrics_metrics = []\n",
    "if not TEST_LYRICS_ARTIST_FINAL_SONGS:\n",
    "    print(\"WARNING: No artist test songs available for generation.\")\n",
    "else:\n",
    "    for original_lyric_text in tqdm(TEST_LYRICS_ARTIST_FINAL_SONGS, desc=f\"Generating & Evaluating (Dropout: {dropout_strategy_for_display})\"):\n",
    "        target_word_count = max(len(nltk.word_tokenize(original_lyric_text)), 20)\n",
    "        if len(original_lyric_text) < SEQUENCE_LENGTH:\n",
    "            seed_text = ' ' * (SEQUENCE_LENGTH - len(original_lyric_text)) + original_lyric_text\n",
    "        else:\n",
    "            seed_text = original_lyric_text[:SEQUENCE_LENGTH]\n",
    "        \n",
    "        generated_lyric_text = generate_text_by_word_count(\n",
    "            model_fine_tuned, tokenizer_general, seed_text, target_word_count,\n",
    "            temperature=0.8, sequence_length=SEQUENCE_LENGTH, device=device\n",
    "        )\n",
    "        \n",
    "        all_generated_lyrics_metrics.append(calculate_text_quality_metrics_updated(generated_lyric_text, existing_words_in_corpus))\n",
    "        all_actual_lyrics_metrics.append(calculate_text_quality_metrics_updated(original_lyric_text, existing_words_in_corpus))\n",
    "\n",
    "    if all_generated_lyrics_metrics:\n",
    "        metric_keys = all_generated_lyrics_metrics[0].keys()\n",
    "        avg_generated_metrics = {k: np.mean([d[k] for d in all_generated_lyrics_metrics]) for k in metric_keys}\n",
    "        avg_actual_metrics = {k: np.mean([d[k] for d in all_actual_lyrics_metrics]) for k in metric_keys}\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"--- MÉDIAS DAS MÉTRICAS DE GERAÇÃO (Dropout: {}) ---\".format(dropout_strategy_for_display))\n",
    "        print(\"--- Letras Geradas (Média) ---\")\n",
    "        for key, value in avg_generated_metrics.items():\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\n--- Letras Originais do Teste (Média) ---\")\n",
    "        for key, value in avg_actual_metrics.items():\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No metrics generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9966e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'wordnet' NLTK package...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Object loaded from: processed_data_general_char_split\\tokenizers\\char_tokenizer_general.pkl\n",
      "Object loaded from: processed_data_general_char_split\\tokenizers\\vocab_size_general_char.pkl\n",
      "General character vocabulary loaded. Vocabulary size: 36\n",
      "spaCy model 'en_core_web_sm' loaded.\n",
      "Size of existing word vocabulary in general corpus (for metrics): 40909\n",
      "\n",
      "Generating prompts and targets for evaluation from 29 exclusive artist test songs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts and targets: 100%|██████████| 29/29 [00:00<00:00, 264.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prompts and targets generated for evaluation: 9916\n",
      "\n",
      "Performing POS Tagging of true words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "POS Tagging true words: 100%|██████████| 10/10 [00:16<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "def calculate_text_quality_metrics(text_to_analyze, reference_words_set):\n",
    "    words = nltk.word_tokenize(text_to_analyze.lower())\n",
    "\n",
    "    metrics = {\n",
    "        'total_words': len(words),\n",
    "        'unique_words': 0,\n",
    "        'ttr': 0.0,\n",
    "        'existing_words_count': 0,\n",
    "        'existence_rate': 0.0,\n",
    "        'repetition_rate_words': 0.0,\n",
    "        'repetition_rate_3grams_char': 0.0\n",
    "    }\n",
    "\n",
    "    if not words:\n",
    "        return metrics\n",
    "\n",
    "    unique_words = set(words)\n",
    "    metrics['unique_words'] = len(unique_words)\n",
    "    metrics['ttr'] = len(unique_words) / len(words)\n",
    "\n",
    "    existing_words_in_generated = 0\n",
    "    for word in unique_words:\n",
    "        if word in reference_words_set:\n",
    "            existing_words_in_generated += 1\n",
    "    metrics['existing_words_count'] = existing_words_in_generated\n",
    "    metrics['existence_rate'] = existing_words_in_generated / len(unique_words) if len(unique_words) > 0 else 0.0\n",
    "\n",
    "    # Repetition of words (proportion of repeated tokens)\n",
    "    metrics['repetition_rate_words'] = (len(words) - len(unique_words)) / len(words) if len(words) > 0 else 0.0\n",
    "\n",
    "    # Repetition of 3-character n-grams\n",
    "    char_3grams = [text_to_analyze[i:i+3] for i in range(len(text_to_analyze) - 2)]\n",
    "    if len(char_3grams) > 0:\n",
    "        unique_char_3grams = set(char_3grams)\n",
    "        metrics['repetition_rate_3grams_char'] = (len(char_3grams) - len(unique_char_3grams)) / len(char_3grams)\n",
    "    else:\n",
    "        metrics['repetition_rate_3grams_char'] = 0.0\n",
    "\n",
    "    return metrics\n",
    "\n",
    "tokenizer_general = None\n",
    "total_chars_general = 0\n",
    "\n",
    "try:\n",
    "    tokenizer_general = load_object(os.path.join(BASE_OUTPUT_DIR_GENERAL, TOKENIZER_SUBDIR, f'char_tokenizer_general.pkl'))\n",
    "    total_chars_general = load_object(os.path.join(BASE_OUTPUT_DIR_GENERAL, TOKENIZER_SUBDIR, f'vocab_size_general_char.pkl'))\n",
    "    print(f\"General character vocabulary loaded. Vocabulary size: {total_chars_general}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: General tokenizer files not found. Details: {e}. Please run preprocessing Phase 1.\")\n",
    "    exit()\n",
    "\n",
    "nlp = None\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy model 'en_core_web_sm' loaded.\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy model 'en_core_web_sm' downloaded and loaded.\")\n",
    "\n",
    "prompts_for_eval = []\n",
    "true_next_words = []\n",
    "true_next_pos_tags = []\n",
    "existing_words_in_corpus = set()\n",
    "\n",
    "try:\n",
    "    all_lyrics_for_vocab_building = []\n",
    "    for filename in os.listdir(CSV_FOLDER_PATH):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            csv_path = os.path.join(CSV_FOLDER_PATH, filename)\n",
    "            df_temp = pd.read_csv(csv_path, usecols=['Lyric'])\n",
    "            for lyric_val in df_temp['Lyric']:\n",
    "                cleaned_lyric = clean_text(lyric_val)\n",
    "                if cleaned_lyric and PLACEHOLDER_PHRASE not in cleaned_lyric:\n",
    "                    all_lyrics_for_vocab_building.append(cleaned_lyric)\n",
    "    \n",
    "    for lyric in all_lyrics_for_vocab_building:\n",
    "        words = nltk.word_tokenize(lyric)\n",
    "        existing_words_in_corpus.update(words)\n",
    "    \n",
    "    print(f\"Size of existing word vocabulary in general corpus (for metrics): {len(existing_words_in_corpus)}\")\n",
    "\n",
    "    ARTIST_LYRICS_CLEAN_ALL = []\n",
    "    df_target_artist_full = pd.read_csv(CSV_FILE_PATH_TARGET_ARTIST, usecols=['Lyric'])\n",
    "    for lyric_val in df_target_artist_full['Lyric']:\n",
    "        cleaned_lyric = clean_text(lyric_val)\n",
    "        if PLACEHOLDER_PHRASE in cleaned_lyric: continue\n",
    "        if cleaned_lyric: ARTIST_LYRICS_CLEAN_ALL.append(cleaned_lyric)\n",
    "    \n",
    "    random.seed(RANDOM_SEED)\n",
    "    random.shuffle(ARTIST_LYRICS_CLEAN_ALL)\n",
    "    \n",
    "    num_test_artist_final = int(len(ARTIST_LYRICS_CLEAN_ALL) * TEST_SPLIT)\n",
    "    TEST_LYRICS_ARTIST_FINAL_FOR_GEN = ARTIST_LYRICS_CLEAN_ALL[:num_test_artist_final]\n",
    "    \n",
    "    if not TEST_LYRICS_ARTIST_FINAL_FOR_GEN:\n",
    "        raise ValueError(\"TEST_LYRICS_ARTIST_FINAL set is empty. Check collection and split in preprocessing.\")\n",
    "    \n",
    "    print(f\"\\nGenerating prompts and targets for evaluation from {len(TEST_LYRICS_ARTIST_FINAL_FOR_GEN)} exclusive artist test songs...\")\n",
    "    for lyric_text in tqdm(TEST_LYRICS_ARTIST_FINAL_FOR_GEN, desc=\"Generating prompts and targets\"):\n",
    "        tokenized_full_lyric = nltk.word_tokenize(lyric_text)\n",
    "        \n",
    "        for target_word_idx in range(1, len(tokenized_full_lyric)):\n",
    "            current_target_word = tokenized_full_lyric[target_word_idx]\n",
    "            preceding_words_list = tokenized_full_lyric[:target_word_idx]\n",
    "            context_string_before_target_word = \" \".join(preceding_words_list) + \" \" \n",
    "\n",
    "            if len(context_string_before_target_word) < SEQUENCE_LENGTH:\n",
    "                final_prompt_chars = ' ' * (SEQUENCE_LENGTH - len(context_string_before_target_word)) + context_string_before_target_word\n",
    "            else:\n",
    "                final_prompt_chars = context_string_before_target_word[-SEQUENCE_LENGTH:]\n",
    "            \n",
    "            if current_target_word:\n",
    "                prompts_for_eval.append(final_prompt_chars)\n",
    "                true_next_words.append(current_target_word)\n",
    "    \n",
    "    print(f\"Total prompts and targets generated for evaluation: {len(prompts_for_eval)}\") \n",
    "    if not prompts_for_eval: \n",
    "        raise ValueError(\"Could not generate enough prompts for evaluation. Check test lyrics length and SEQUENCE_LENGTH.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading test data or preparing prompts: {e}\")\n",
    "    prompts_for_eval = []\n",
    "    true_next_words = []\n",
    "    true_next_pos_tags = [] \n",
    "    existing_words_in_corpus = set() \n",
    "    pass \n",
    "\n",
    "if true_next_words and nlp:\n",
    "    print(\"\\nPerforming POS Tagging of true words...\")\n",
    "    batch_size_spacy = 1000\n",
    "    for i in tqdm(range(0, len(true_next_words), batch_size_spacy), desc=\"POS Tagging true words\"):\n",
    "        batch_words = true_next_words[i:i + batch_size_spacy]\n",
    "        docs = list(nlp.pipe(batch_words))\n",
    "        for doc in docs:\n",
    "            true_next_pos_tags.append(doc[0].pos_ if doc and doc[0] else \"UNKNOWN\")\n",
    "else:\n",
    "    print(\"No true words to perform POS Tagging or spaCy model not loaded. Skipping POS Tagging.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26cea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 1: Word-Level Evaluations for Fine-tuned Model (ALL Test Prompts) ---\n",
      "Fine-tuned model loaded from: models_by_artist_char_split_finetuned\\arianagrande\\model_weights_arianagrande_finetuned_strategy_none_best.pt\n",
      "\n",
      "Starting grammatical cohesion and next-word accuracy evaluation for 9916 test prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Next Word & POS Tag: 100%|██████████| 9916/9916 [11:24<00:00, 14.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results: Grammatical Cohesion (POS) & Next Word Accuracy (Fine-tuned Model) ---\n",
      "Total prompts evaluated: 9916\n",
      "Correct POS Tag Predictions: 5037\n",
      "Overall Next POS Tag Accuracy: 50.80%\n",
      "Correct Next Word Predictions: 2968\n",
      "Overall Next Word Prediction Accuracy: 29.93%\n",
      "\n",
      "--- POS Tag Accuracy per True POS Tag ---\n",
      "  When true POS is 'ADP': 33.56% (Correct: 200/596)\n",
      "  When true POS is 'PROPN': 27.56% (Correct: 113/410)\n",
      "  When true POS is 'PRON': 76.76% (Correct: 2120/2762)\n",
      "  When true POS is 'VERB': 52.46% (Correct: 980/1868)\n",
      "  When true POS is 'AUX': 44.47% (Correct: 213/479)\n",
      "  When true POS is 'NOUN': 47.96% (Correct: 600/1251)\n",
      "  When true POS is 'ADJ': 34.71% (Correct: 118/340)\n",
      "  When true POS is 'INTJ': 38.13% (Correct: 159/417)\n",
      "  When true POS is 'PUNCT': 2.56% (Correct: 6/234)\n",
      "  When true POS is 'SCONJ': 30.69% (Correct: 58/189)\n",
      "  When true POS is 'ADV': 41.28% (Correct: 251/608)\n",
      "  When true POS is 'CCONJ': 33.88% (Correct: 82/242)\n",
      "  When true POS is 'PART': 19.06% (Correct: 69/362)\n",
      "  When true POS is 'NUM': 53.17% (Correct: 67/126)\n",
      "  When true POS is 'X': 3.12% (Correct: 1/32)\n",
      "\n",
      "--- Simplified POS Tag Confusion Matrix (True \\ Predicted) ---\n",
      "True \\ Pred    UNKNOWN   NONE      ADJ       ADP       ADV       AUX       CCONJ     INTJ      NOUN      NUM       PART      PRON      PROPN     PUNCT     SCONJ     VERB      X         \n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "UNKNOWN        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         \n",
      "NONE           0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         \n",
      "ADJ            0         0         118       28        22        7         3         11        50        4         4         65        2         0         2         19        5         \n",
      "ADP            0         0         6         200       36        10        18        14        27        0         53        158       13        0         23        38        0         \n",
      "ADV            0         0         9         32        251       22        21        20        38        2         18        117       7         0         8         62        1         \n",
      "AUX            0         0         0         4         8         213       4         11        57        0         0         50        6         0         1         125       0         \n",
      "CCONJ          0         0         5         19        7         5         82        15        25        0         13        46        10        0         1         14        0         \n",
      "INTJ           0         0         1         17        21        12        15        159       42        1         4         82        31        0         6         26        0         \n",
      "NOUN           0         0         39        35        48        23        29        81        600       21        14        221       30        0         16        91        3         \n",
      "NUM            0         0         2         2         3         2         4         4         7         67        1         24        4         0         2         4         0         \n",
      "PART           0         0         3         7         18        38        9         3         20        1         69        119       3         0         5         67        0         \n",
      "PRON           0         0         13        76        69        30        79        58        80        2         51        2120      31        0         76        75        2         \n",
      "PROPN          0         0         4         13        19        8         10        26        44        1         6         84        113       0         3         76        3         \n",
      "PUNCT          0         0         0         17        28        1         20        26        12        0         12        76        9         6         3         24        0         \n",
      "SCONJ          0         0         1         4         1         0         7         9         6         0         4         84        5         0         58        10        0         \n",
      "VERB           0         0         56        46        80        166       37        53        129       11        19        240       35        1         15        980       0         \n",
      "X              0         0         0         0         18        0         1         0         2         0         0         3         2         0         0         5         1         \n",
      "-----------------------------------------------------\n",
      "\n",
      "--- Metrics for GENERATED WORDS (from next-word prediction task) ---\n",
      "Total Generated Words: 10487\n",
      "Unique Generated Words: 629\n",
      "TTR (Generated Words): 0.0600\n",
      "Repetition Rate (Generated Words): 0.9400\n",
      "Generated Unique Words Existing in Corpus: 616\n",
      "Existence Rate (Generated Unique Words): 0.98%\n",
      "-----------------------------------------------------\n",
      "\n",
      "--- Metrics for ACTUAL TEST WORDS (from artist's test set) ---\n",
      "Total Actual Test Words: 9916\n",
      "Unique Actual Test Words: 998\n",
      "TTR (Actual Test Words): 0.1006\n",
      "Repetition Rate (Actual Test Words): 0.8994\n",
      "Actual Unique Words Existing in Corpus: 998\n",
      "Existence Rate (Actual Unique Words): 1.00%\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Executa uma avaliação granular do modelo fine-tuned, focada na predição da\n",
    "próxima palavra. Carrega o modelo fine-tuned e itera sobre os prompts de teste\n",
    "preparados na célula anterior. Calcula e exibe a acurácia da predição da\n",
    "próxima palavra, a acurácia da etiqueta gramatical (POS tag), uma análise\n",
    "de acurácia por classe gramatical e uma matriz de confusão. Adicionalmente,\n",
    "compara as métricas de qualidade de texto (TTR, etc.) entre o conjunto de todas\n",
    "as palavras geradas e o conjunto de todas as palavras reais do teste.\n",
    "\"\"\"\n",
    "print(\"\\n--- Avaliação de Nível de Palavra para o Modelo Fine-Tuned ---\")\n",
    "\n",
    "CURRENT_DROPOUT_STRATEGY_FINE_TUNE = \"none\"\n",
    "model_fine_tune_save_filepath = os.path.join(MODEL_DIR_BASE_ARTIST_FINE_TUNED, CLEAN_TARGET_ARTIST_NAME, f\"model_weights_{CLEAN_TARGET_ARTIST_NAME}_finetuned_strategy_{CURRENT_DROPOUT_STRATEGY_FINE_TUNE}_best.pt\")\n",
    "\n",
    "try:\n",
    "    model_fine_tune_eval = CharLSTM(\n",
    "        vocab_size=total_chars_general,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        dropout_strategy=CURRENT_DROPOUT_STRATEGY_FINE_TUNE\n",
    "    ).to(device)\n",
    "    model_fine_tune_eval.load_state_dict(torch.load(model_fine_tune_save_filepath, map_location=device))\n",
    "    model_fine_tune_eval.eval()\n",
    "    print(f\"Modelo fine-tuned carregado de: {model_fine_tune_save_filepath}\")\n",
    "\n",
    "    if not prompts_for_eval or not true_next_words or not nlp:\n",
    "        print(\"\\nAviso: Dados ou modelo spaCy insuficientes para avaliação de nível de palavra. Pulando.\")\n",
    "    else:\n",
    "        correct_pos_predictions = 0\n",
    "        correct_word_predictions = 0\n",
    "        pos_confusion_matrix = {}\n",
    "        generated_words_list = []\n",
    "        generated_pos_tags_list = []\n",
    "\n",
    "        print(f\"\\nIniciando avaliação de coesão gramatical e acurácia para {len(prompts_for_eval)} prompts...\")\n",
    "        for i in tqdm(range(len(prompts_for_eval)), desc=\"Avaliando Próxima Palavra & POS Tag\"):\n",
    "            prompt = prompts_for_eval[i]\n",
    "            true_word = true_next_words[i]\n",
    "            true_pos = true_next_pos_tags[i]\n",
    "\n",
    "            # Esta função de geração determinística é definida na célula de setup\n",
    "            generated_word = generate_text_by_word_count(model_fine_tune_eval, tokenizer_general, prompt, len(nltk.word_tokenize(prompt)) + 1, temperature=0, sequence_length=SEQUENCE_LENGTH, device=device).split()[-1]\n",
    "            generated_words_list.append(generated_word)\n",
    "\n",
    "            generated_pos = \"NONE\" if not generated_word else (nlp(generated_word)[0].pos_ if nlp(generated_word) and nlp(generated_word)[0] else \"UNKNOWN\")\n",
    "            generated_pos_tags_list.append(generated_pos)\n",
    "\n",
    "            pos_confusion_matrix.setdefault(true_pos, {}).setdefault(generated_pos, 0)\n",
    "            pos_confusion_matrix[true_pos][generated_pos] += 1\n",
    "            if generated_pos == true_pos:\n",
    "                correct_pos_predictions += 1\n",
    "            \n",
    "            cleaned_true_word = re.sub(r'[^a-z0-9]', '', true_word).lower()\n",
    "            cleaned_generated_word = re.sub(r'[^a-z0-9]', '', generated_word).lower()\n",
    "            if cleaned_generated_word == cleaned_true_word and cleaned_true_word:\n",
    "                correct_word_predictions += 1\n",
    "\n",
    "        total_predictions = len(prompts_for_eval)\n",
    "        overall_pos_accuracy = (correct_pos_predictions / total_predictions) * 100 if total_predictions > 0 else 0.0\n",
    "        overall_word_accuracy = (correct_word_predictions / total_predictions) * 100 if total_predictions > 0 else 0.0\n",
    "\n",
    "        print(f\"\\nResultados de Coesão Gramatical (POS) & Acurácia da Próxima Palavra (Modelo Fine-tuned)\")\n",
    "        print(f\"Acurácia Geral da Próxima Etiqueta POS: {overall_pos_accuracy:.2f}%\")\n",
    "        print(f\"Acurácia Geral da Próxima Palavra: {overall_word_accuracy:.2f}%\")\n",
    "\n",
    "        print(\"\\nMétricas para PALAVRAS GERADAS (tarefa de predição)\")\n",
    "        generated_metrics = calculate_text_quality_metrics(\" \".join(generated_words_list), existing_words_in_corpus)\n",
    "        for key, value in generated_metrics.items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "        print(\"\\nMétricas para PALAVRAS REAIS (do conjunto de teste)\")\n",
    "        actual_metrics = calculate_text_quality_metrics(\" \".join(true_next_words), existing_words_in_corpus)\n",
    "        for key, value in actual_metrics.items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Modelo fine-tuned não encontrado em '{model_fine_tune_save_filepath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1d2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe gramatical mais comum: 'PRON' (2762 ocorrências)\n",
      "**Acurácia do Baseline (sempre prevendo a mais comum): 27.85%**\n",
      "\n",
      "Probabilidades e Contribuições para o Baseline Aleatório Ponderado:\n",
      "  'ADP': Probabilidade = 0.0601, Contribuição = 0.0036\n",
      "  'PROPN': Probabilidade = 0.0413, Contribuição = 0.0017\n",
      "  'PRON': Probabilidade = 0.2785, Contribuição = 0.0776\n",
      "  'VERB': Probabilidade = 0.1884, Contribuição = 0.0355\n",
      "  'AUX': Probabilidade = 0.0483, Contribuição = 0.0023\n",
      "  'NOUN': Probabilidade = 0.1262, Contribuição = 0.0159\n",
      "  'ADJ': Probabilidade = 0.0343, Contribuição = 0.0012\n",
      "  'INTJ': Probabilidade = 0.0421, Contribuição = 0.0018\n",
      "  'PUNCT': Probabilidade = 0.0236, Contribuição = 0.0006\n",
      "  'SCONJ': Probabilidade = 0.0191, Contribuição = 0.0004\n",
      "  'ADV': Probabilidade = 0.0613, Contribuição = 0.0038\n",
      "  'CCONJ': Probabilidade = 0.0244, Contribuição = 0.0006\n",
      "  'PART': Probabilidade = 0.0365, Contribuição = 0.0013\n",
      "  'NUM': Probabilidade = 0.0127, Contribuição = 0.0002\n",
      "  'X': Probabilidade = 0.0032, Contribuição = 0.0000\n",
      "**Acurácia do Baseline (chutar aleatoriamente com base na distribuição): 14.64%**\n",
      "\n",
      "Sua acurácia (Overall Next POS Tag Accuracy): 50.80%\n",
      "O modelo (50.80%) é **melhor** que o baseline de maioria (27.85%).\n",
      "O modelo (50.80%) é **melhor** que o baseline aleatório ponderado (14.64%).\n"
     ]
    }
   ],
   "source": [
    "if not true_next_pos_tags:\n",
    "    print(\"A lista 'true_next_pos_tags' está vazia. Não é possível calcular baselines.\")\n",
    "else:\n",
    "    pos_counts = Counter(true_next_pos_tags)\n",
    "    total_tags = len(true_next_pos_tags)\n",
    "\n",
    "    # --- Calcular o Baseline (chutar sempre o mais comum) ---\n",
    "    most_common_pos, most_common_count = pos_counts.most_common(1)[0]\n",
    "    baseline_majority_accuracy = (most_common_count / total_tags) * 100\n",
    "    print(f\"Classe gramatical mais comum: '{most_common_pos}' ({most_common_count} ocorrências)\")\n",
    "    print(f\"**Acurácia do Baseline (sempre prevendo a mais comum): {baseline_majority_accuracy:.2f}%**\")\n",
    "\n",
    "    # --- Calcular o Baseline (chutar aleatoriamente baseado na distribuição) ---\n",
    "    expected_random_accuracy = 0.0\n",
    "    print(\"\\nProbabilidades e Contribuições para o Baseline Aleatório Ponderado:\")\n",
    "    for pos_tag, count in pos_counts.items():\n",
    "        probability = count / total_tags\n",
    "        contribution = probability ** 2\n",
    "        expected_random_accuracy += contribution\n",
    "        print(f\"  '{pos_tag}': Probabilidade = {probability:.4f}, Contribuição = {contribution:.4f}\")\n",
    "\n",
    "    expected_random_accuracy_percent = expected_random_accuracy * 100\n",
    "    print(f\"**Acurácia do Baseline (chutar aleatoriamente com base na distribuição): {expected_random_accuracy_percent:.2f}%**\")\n",
    "\n",
    "    # --- Comparação com o seu resultado (50.80%) ---\n",
    "    your_model_accuracy = 50.80\n",
    "    print(f\"\\nSua acurácia (Overall Next POS Tag Accuracy): {your_model_accuracy:.2f}%\")\n",
    "\n",
    "    if your_model_accuracy > baseline_majority_accuracy:\n",
    "        print(f\"O modelo ({your_model_accuracy:.2f}%) é melhor que o baseline de maioria ({baseline_majority_accuracy:.2f}%).\")\n",
    "    else:\n",
    "        print(f\"O modelo ({your_model_accuracy:.2f}%) é pior ou igual ao baseline de maioria ({baseline_majority_accuracy:.2f}%).\")\n",
    "\n",
    "    if your_model_accuracy > expected_random_accuracy_percent:\n",
    "        print(f\"O modelo ({your_model_accuracy:.2f}%) é melhor que o baseline aleatório ponderado ({expected_random_accuracy_percent:.2f}%).\")\n",
    "    else:\n",
    "        print(f\"O modelo ({your_model_accuracy:.2f}%) é pior ou igual ao baseline aleatório ponderado ({expected_random_accuracy_percent:.2f}%).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc6b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Avalia a qualidade do texto longo gerado pelo modelo fine-tuned, focando em\n",
    "métricas de nível de caractere. Define funções para gerar texto longo,\n",
    "calcular o Type-Token Ratio (TTR) de caracteres e a taxa de repetição de\n",
    "n-gramas. Carrega o modelo fine-tuned, gera uma única amostra de texto\n",
    "longa e, por fim, calcula e compara as métricas de diversidade e repetição\n",
    "do texto gerado com as do conjunto de teste real do artista.\n",
    "\"\"\"\n",
    "print(\"\\n--- Avaliação de Geração de Texto Longo (Nível de Caractere) ---\")\n",
    "\n",
    "def generate_long_text(model, tokenizer, seed_text, num_generate_chars=1000, temperature=0.8, sequence_length=SEQUENCE_LENGTH, device=device):\n",
    "    model.eval()\n",
    "    generated_text = seed_text\n",
    "    for _ in tqdm(range(num_generate_chars), desc=\"Gerando texto longo\"):\n",
    "        current_sequence_str = generated_text[-sequence_length:]\n",
    "        tokenized_input = tokenizer.texts_to_sequences(current_sequence_str)\n",
    "        if not tokenized_input or all(t == 0 for t in tokenized_input):\n",
    "            break\n",
    "        input_tensor = torch.tensor(tokenized_input, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor).squeeze(0)\n",
    "        \n",
    "        if temperature <= 0:\n",
    "            next_char_id = torch.argmax(predictions, dim=-1).item()\n",
    "        else:\n",
    "            probabilities = torch.softmax(predictions / temperature, dim=-1)\n",
    "            next_char_id = torch.multinomial(probabilities, 1).item()\n",
    "        \n",
    "        generated_text += tokenizer.int_to_char.get(next_char_id, '')\n",
    "    return generated_text\n",
    "\n",
    "def calculate_char_ttr(text):\n",
    "    if not text: return 0.0\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "def calculate_ngram_repetition_rate(text, n=3):\n",
    "    if len(text) < n: return 0.0\n",
    "    ngrams = [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "    if not ngrams: return 0.0\n",
    "    return (len(ngrams) - len(set(ngrams))) / len(ngrams)\n",
    "\n",
    "CURRENT_DROPOUT_STRATEGY_FINE_TUNE = \"none\"\n",
    "model_fine_tune_save_filepath = os.path.join(MODEL_DIR_BASE_ARTIST_FINE_TUNED, CLEAN_TARGET_ARTIST_NAME, f\"model_weights_{CLEAN_TARGET_ARTIST_NAME}_finetuned_strategy_{CURRENT_DROPOUT_STRATEGY_FINE_TUNE}_best.pt\")\n",
    "\n",
    "try:\n",
    "    model_fine_tune_eval_long_gen = CharLSTM(\n",
    "        vocab_size=total_chars_general,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        dropout_strategy=CURRENT_DROPOUT_STRATEGY_FINE_TUNE\n",
    "    ).to(device)\n",
    "    model_fine_tune_eval_long_gen.load_state_dict(torch.load(model_fine_tune_save_filepath, map_location=device))\n",
    "    model_fine_tune_eval_long_gen.eval()\n",
    "    print(f\"Modelo fine-tuned carregado de: {model_fine_tune_save_filepath}\")\n",
    "\n",
    "    NUM_CHARS_FOR_LONG_TEXT_GEN = 5000\n",
    "    GENERATION_TEMPERATURE = 0.8\n",
    "    print(f\"\\nGerando texto longo de {NUM_CHARS_FOR_LONG_TEXT_GEN} caracteres...\")\n",
    "\n",
    "    if not TEST_LYRICS_ARTIST_FINAL_FOR_GEN:\n",
    "        print(\"Aviso: Nenhuma música de teste disponível para usar como seed.\")\n",
    "        generated_long_text_fine_tune = \"\"\n",
    "    else:\n",
    "        random.seed(RANDOM_SEED)\n",
    "        idx_for_seed = random.randint(0, len(TEST_LYRICS_ARTIST_FINAL_FOR_GEN) - 1)\n",
    "        sample_seed_text = clean_text(TEST_LYRICS_ARTIST_FINAL_FOR_GEN[idx_for_seed])\n",
    "        \n",
    "        if len(sample_seed_text) < SEQUENCE_LENGTH:\n",
    "            sample_seed_text = ' ' * (SEQUENCE_LENGTH - len(sample_seed_text)) + sample_seed_text\n",
    "        else:\n",
    "            sample_seed_text = sample_seed_text[-SEQUENCE_LENGTH:]\n",
    "        \n",
    "        print(f\"Seed inicial: '{sample_seed_text}'\")\n",
    "        generated_long_text_fine_tune = generate_long_text(\n",
    "            model_fine_tune_eval_long_gen, tokenizer_general, sample_seed_text,\n",
    "            num_generate_chars=NUM_CHARS_FOR_LONG_TEXT_GEN,\n",
    "            temperature=GENERATION_TEMPERATURE,\n",
    "            sequence_length=SEQUENCE_LENGTH,\n",
    "            device=device\n",
    "        )\n",
    "        print(f\"Amostra Gerada (início): '{generated_long_text_fine_tune[:200]}...'\")\n",
    "\n",
    "        print(\"\\nMétricas de Nível de Caractere (Texto Gerado)\")\n",
    "        print(f\"  TTR de Caracteres: {calculate_char_ttr(generated_long_text_fine_tune):.4f}\")\n",
    "        print(f\"  Taxa de Repetição de 3-gramas: {calculate_ngram_repetition_rate(generated_long_text_fine_tune, n=3):.4f}\")\n",
    "\n",
    "    if TEST_LYRICS_ARTIST_FINAL_FOR_GEN:\n",
    "        print(\"\\nMétricas de Nível de Caractere (Conjunto de Teste Real do Artista)\")\n",
    "        combined_actual_test_lyrics = \" \".join([clean_text(lyric) for lyric in TEST_LYRICS_ARTIST_FINAL_FOR_GEN])\n",
    "        print(f\"  TTR de Caracteres: {calculate_char_ttr(combined_actual_test_lyrics):.4f}\")\n",
    "        print(f\"  Taxa de Repetição de 3-gramas: {calculate_ngram_repetition_rate(combined_actual_test_lyrics, n=3):.4f}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Modelo fine-tuned não encontrado em '{model_fine_tune_save_filepath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670b098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 3: Loss and Accuracy Evaluation (General & Fine-tuned Models) ---\n",
      "\n",
      "--- ACCURACY & LOSS EVALUATION: Geral Model (Dropout: 'none') ---\n",
      "Model 'model_weights_general_strategy_none_best.pt' loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Loss/Acc (Geral): 100%|██████████| 3709/3709 [03:58<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results: Loss & Accuracy (Geral - Dropout: none) ---\n",
      "Loss (Cross-Entropy): 1.1837\n",
      "Accuracy (Next Character): 0.6324\n",
      "Perplexity: 3.2664\n",
      "-----------------------------------------------------\n",
      "\n",
      "--- ACCURACY & LOSS EVALUATION: Fine-tuned Model (Dropout: 'none') ---\n",
      "Model 'model_weights_arianagrande_finetuned_strategy_none_best.pt' loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Loss/Acc (Fine-tuned): 100%|██████████| 165/165 [00:10<00:00, 15.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results: Loss & Accuracy (Fine-tuned - Dropout: none) ---\n",
      "Loss (Cross-Entropy): 0.7528\n",
      "Accuracy (Next Character): 0.7932\n",
      "Perplexity: 2.1230\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define e executa uma função de avaliação padronizada para calcular as métricas\n",
    "de desempenho fundamentais (Perda, Acurácia, Perplexidade). A função é\n",
    "projetada para ser reutilizável, carregando o modelo e o conjunto de teste\n",
    "apropriados com base no tipo de modelo especificado ('Geral' ou 'Fine-tuned').\n",
    "Por fim, a célula executa esta avaliação tanto para o modelo geral quanto\n",
    "para o modelo fine-tuned, exibindo seus resultados.\n",
    "\"\"\"\n",
    "print(\"\\n--- Avaliação de Perda e Acurácia (Modelos Geral e Fine-tuned) ---\")\n",
    "\n",
    "def run_accuracy_loss_evaluation(model_type, dropout_strategy_to_load, tokenizer_obj, device_obj):\n",
    "    \"\"\"Executa a avaliação de perda e acurácia para um tipo de modelo e estratégia de dropout.\"\"\"\n",
    "    print(f\"\\nAvaliando: Modelo {model_type} (Dropout: '{dropout_strategy_to_load}')\")\n",
    "\n",
    "    if model_type == \"Geral\":\n",
    "        model_dir = MODEL_DIR_BASE_GENERAL\n",
    "        model_filename = f\"model_weights_general_strategy_{dropout_strategy_to_load}_best.pt\"\n",
    "        X_test_path = os.path.join(BASE_OUTPUT_DIR_GENERAL, DATA_SUBDIR, 'X_test_general_char.npy')\n",
    "        y_test_path = os.path.join(BASE_OUTPUT_DIR_GENERAL, DATA_SUBDIR, 'y_test_general_char.npy')\n",
    "    elif model_type == \"Fine-tuned\":\n",
    "        model_dir = os.path.join(MODEL_DIR_BASE_ARTIST_FINE_TUNED, CLEAN_TARGET_ARTIST_NAME)\n",
    "        model_filename = f\"model_weights_{CLEAN_TARGET_ARTIST_NAME}_finetuned_strategy_{dropout_strategy_to_load}_best.pt\"\n",
    "        X_test_path = os.path.join(BASE_OUTPUT_DIR_ARTIST, CLEAN_TARGET_ARTIST_NAME, DATA_SUBDIR, f'X_test_{CLEAN_TARGET_ARTIST_NAME}_char.npy')\n",
    "        y_test_path = os.path.join(BASE_OUTPUT_DIR_ARTIST, CLEAN_TARGET_ARTIST_NAME, DATA_SUBDIR, f'y_test_{CLEAN_TARGET_ARTIST_NAME}_char.npy')\n",
    "    else:\n",
    "        print(f\"Tipo de modelo inválido: {model_type}\")\n",
    "        return\n",
    "\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    if not os.path.exists(model_filepath):\n",
    "        print(f\"  - Erro: Modelo não encontrado em '{model_filepath}'.\")\n",
    "        return\n",
    "\n",
    "    model = CharLSTM(\n",
    "        vocab_size=total_chars_general, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM,\n",
    "        n_layers=N_LAYERS, dropout_rate=DROPOUT_RATE, dropout_strategy=dropout_strategy_to_load\n",
    "    ).to(device_obj)\n",
    "    model.load_state_dict(torch.load(model_filepath, map_location=device_obj))\n",
    "    model.eval()\n",
    "    print(f\"  - Modelo '{model_filename}' carregado.\")\n",
    "\n",
    "    if not (os.path.exists(X_test_path) and os.path.exists(y_test_path)):\n",
    "        print(f\"  - Aviso: Dados de teste não encontrados em '{X_test_path}'.\")\n",
    "        return\n",
    "\n",
    "    test_dataset = LyricsDataset(X_test_path, y_test_path)\n",
    "    if len(test_dataset) == 0:\n",
    "        print(f\"  - Aviso: Dataset de teste em '{X_test_path}' está vazio.\")\n",
    "        return\n",
    "        \n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss, correct_predictions, total_samples = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=f\"Calculando Loss/Acc ({model_type})\"):\n",
    "            inputs, labels = inputs.to(device_obj), labels.to(device_obj)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples if total_samples > 0 else float('nan')\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else float('nan')\n",
    "    perplexity = np.exp(avg_loss) if not np.isnan(avg_loss) else float('nan')\n",
    "\n",
    "    print(f\"  - Perda (Cross-Entropy): {avg_loss:.4f}\")\n",
    "    print(f\"  - Acurácia (Próximo Caractere): {accuracy:.4f}\")\n",
    "    print(f\"  - Perplexidade: {perplexity:.4f}\")\n",
    "\n",
    "# Executa a avaliação para ambos os modelos com a estratégia de dropout 'none'\n",
    "run_accuracy_loss_evaluation(\"Geral\", \"none\", tokenizer_general, device)\n",
    "run_accuracy_loss_evaluation(\"Fine-tuned\", \"none\", tokenizer_general, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6ca92",
   "metadata": {},
   "source": [
    "## Comparação das letras geradas a partir de prompts do teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaaf194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Objeto carregado de: processed_data_general_char_split\\tokenizers\\char_tokenizer_general.pkl\n",
      "Objeto carregado de: processed_data_general_char_split\\tokenizers\\vocab_size_general_char.pkl\n",
      "Vocabulário de caracteres GERAL carregado. Tamanho do vocabulário: 36\n",
      "Modelo Geral carregado de: models_general_char_split\\model_weights_general_strategy_none_best.pt\n",
      "Modelo Fine-tuned carregado de: models_by_artist_char_split_finetuned\\arianagrande\\model_weights_arianagrande_finetuned_strategy_none_best.pt\n",
      "\n",
      "Coletadas 29 músicas do artista 'ArianaGrande' para seleção de prompts.\n",
      "\n",
      "--- Comparação de Geração de Texto (Modelo Geral vs. Fine-tuned) ---\n",
      "Artista Alvo: ArianaGrande\n",
      "Quantidade de Exemplos: 10\n",
      "Caracteres Gerados por Modelo: 100\n",
      "Temperatura de Geração: 0\n",
      "================================================================================\n",
      "\n",
      "--- Exemplo 1 ---\n",
      "PROMPT DE ENTRADA (100 caracteres):\n",
      "'is the part when i say i don't want ya i'm stronger than i've been before this is the part when i br'\n",
      "\n",
      "CONTINUAÇÃO REAL:\n",
      "'eak free 'cause i can't resist it no more drop you were better deeper i was under your spell yeah li'\n",
      "\n",
      "GERADO PELO MODELO GERAL:\n",
      "'ing you're bein' a man that basters can you leave me don't get me roller and my open man don't call '\n",
      "\n",
      "GERADO PELO MODELO FINE-TUNED:\n",
      "'eak free 'cause i can't resist it no more even when i'm striptin' on a river ariana grande i'm tryna'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Exemplo 2 ---\n",
      "PROMPT DE ENTRADA (100 caracteres):\n",
      "'i thought that i was a hopeless romantic i thought that i was a hopeless romantic i really thought t'\n",
      "\n",
      "CONTINUAÇÃO REAL:\n",
      "'hat i'd be like my grandparents i thought that i was a hopeless romantic'\n",
      "\n",
      "GERADO PELO MODELO GERAL:\n",
      "'he disaase is switchin's no emba thicker swinkin' i had clapsed in a music i never only meant to tel'\n",
      "\n",
      "GERADO PELO MODELO FINE-TUNED:\n",
      "'hat post hear my guy got it single im gonna regret it i know i really got it hey you got it you got '\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Exemplo 3 ---\n",
      "PROMPT DE ENTRADA (100 caracteres):\n",
      "'it made me feel you know the drill one night and you feel fulfilled this is the thrill ariana grande'\n",
      "\n",
      "CONTINUAÇÃO REAL:\n",
      "' can you handle me i think you're fuckin' with me with me i know you wanna kiss me kiss me i know yo'\n",
      "\n",
      "GERADO PELO MODELO GERAL:\n",
      "''s then i'm seeing yeah this in the bedroom to life i'm like i'm the way that all the chance of reci'\n",
      "\n",
      "GERADO PELO MODELO FINE-TUNED:\n",
      "' magic ridiculous said this iel about ridicules yeah yeah yeah ariana grande the same baby make them'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Exemplo 4 ---\n",
      "PROMPT DE ENTRADA (100 caracteres):\n",
      "'her own you know you know you know you know she got her own ladies in the buildin' ladies in the she'\n",
      "\n",
      "CONTINUAÇÃO REAL:\n",
      "' got her own she got her own ladies in the buildin' ladies in the buildin' she got her own she got h'\n",
      "\n",
      "GERADO PELO MODELO GERAL:\n",
      "'et you'll go dromed it's gettin' all inside yellowing they put like physetical but you'll ask me lov'\n",
      "\n",
      "GERADO PELO MODELO FINE-TUNED:\n",
      "'et looking so crazy right back low far fashion as selfest face somewhere in a word it's wrong when a'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Exemplo 5 ---\n",
      "PROMPT DE ENTRADA (100 caracteres):\n",
      "'u tell me tell me baby why can't you leave me 'cause even though i shouldn't want it i gotta have it'\n",
      "\n",
      "CONTINUAÇÃO REAL:\n",
      "' i want you pre head in the clouds got no weight on my shoulders i should be wiser and realize that '\n",
      "\n",
      "GERADO PELO MODELO GERAL:\n",
      "' i see in you surrate and hippy now i'm not wearen i'm the one of you like a bow i'm kind of trender'\n",
      "\n",
      "GERADO PELO MODELO FINE-TUNED:\n",
      "' won't be happy in the end i won't be simple i'm paid i've got one way too you been your touch there'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Exemplo 6 ---\n",
      "PROMPT DE ENTRADA (100 caracteres):\n",
      "'xt day you gave it away this year to save me from tears i'll give it to someone special this is our '\n",
      "\n",
      "CONTINUAÇÃO REAL:\n",
      "'last last christmas you broke my heart last last christmas you broke my heart this year to save me f'\n",
      "\n",
      "GERADO PELO MODELO GERAL:\n",
      "' knees for vatter at this return or lie i'm contend the stole's over you're not trusting to whip clo'\n",
      "\n",
      "GERADO PELO MODELO FINE-TUNED:\n",
      "' superstar every tear is easy on my hundred all that i've been so in loven i'm loving it but i'm pic'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Exemplo 7 ---\n",
      "PROMPT DE ENTRADA (100 caracteres):\n",
      "'his is my song i'm the biggest bang with the mostest zing if you're the hand i'm the diamond ring by'\n",
      "\n",
      "CONTINUAÇÃO REAL:\n",
      "' the way i'm even the song and the dance so give me one give me one i've already won all i i need ju'\n",
      "\n",
      "GERADO PELO MODELO GERAL:\n",
      "' my girl filved stuck take me told me i'm switching the all the tend this is got the lyrna of your l'\n",
      "\n",
      "GERADO PELO MODELO FINE-TUNED:\n",
      "' the kitchen yeah who's true trying to say i'm burning up with my magic ticated of bottom whats up a'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Exemplo 8 ---\n",
      "PROMPT DE ENTRADA (100 caracteres):\n",
      "'ugh i can't forgive ya i really want to i want you tell me tell me baby why can't you leave me 'caus'\n",
      "\n",
      "CONTINUAÇÃO REAL:\n",
      "'e even though i shouldn't want it i gotta have it i want you pre ariana grande head in the clouds go'\n",
      "\n",
      "GERADO PELO MODELO GERAL:\n",
      "'e i can't believe no no i'm cetcanting to be a mongem in crazy that's some four on the outfors hip i'\n",
      "\n",
      "GERADO PELO MODELO FINE-TUNED:\n",
      "'e even though i let you be my baby my favorite southout to see me strike a signa song everyhe please'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Exemplo 9 ---\n",
      "PROMPT DE ENTRADA (100 caracteres):\n",
      "'welcome to this christmas only comes one time a year don't miss this i just wanna welcome you to chr'\n",
      "\n",
      "CONTINUAÇÃO REAL:\n",
      "'istmas let me sneak into your speakers listen i'll be singing oh oh oh oh i'll be singing oh oh oh o'\n",
      "\n",
      "GERADO PELO MODELO GERAL:\n",
      "'istmas post tat that on don't die for got lovin attack who'lves no way you need me i'm on that yeah '\n",
      "\n",
      "GERADO PELO MODELO FINE-TUNED:\n",
      "'istmas catch me how did i hang up your mind that makes me cry baby carry you gotta take my hands ari'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Exemplo 10 ---\n",
      "PROMPT DE ENTRADA (100 caracteres):\n",
      "'cept this wine babe saving up my energy yeah yeah yeah can you stay up all night all night fuck me ''\n",
      "\n",
      "CONTINUAÇÃO REAL:\n",
      "'til the daylight daylight thirtyfour thirtyfive thirtyfive yeah yeah yeah yeah can you stay up all n'\n",
      "\n",
      "GERADO PELO MODELO GERAL:\n",
      "'cause all my realy billie maybe baby my mind is voiced with a noda then my pennysache are you appame'\n",
      "\n",
      "GERADO PELO MODELO FINE-TUNED:\n",
      "'til the daylight the summertime i see the night and i keep finish feeling him and go drop sinner tha'\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Realiza uma comparação qualitativa lado a lado entre o modelo geral e o modelo\n",
    "fine-tuned. Define uma função de geração de texto, carrega o tokenizador e os\n",
    "pesos de ambos os modelos, e prepara uma lista de prompts a partir do conjunto\n",
    "de teste exclusivo do artista. Em seguida, para vários exemplos aleatórios,\n",
    "exibe o prompt, a continuação real da música e as continuações geradas por\n",
    "cada um dos dois modelos, permitindo uma análise visual da melhoria obtida com\n",
    "o fine-tuning.\n",
    "\"\"\"\n",
    "def generate_text_char_level(model, tokenizer, seed_text, num_generate=500, temperature=0.8, sequence_length=SEQUENCE_LENGTH, device=device):\n",
    "    model.eval()\n",
    "    processed_seed_text = clean_text(seed_text)\n",
    "    if len(processed_seed_text) < sequence_length:\n",
    "        processed_seed_text = ' ' * (sequence_length - len(processed_seed_text)) + processed_seed_text\n",
    "    elif len(processed_seed_text) > sequence_length:\n",
    "        processed_seed_text = processed_seed_text[-sequence_length:]\n",
    "    \n",
    "    generated_text = processed_seed_text\n",
    "    for _ in range(num_generate):\n",
    "        current_sequence_str = generated_text[-sequence_length:]\n",
    "        tokenized_input = tokenizer.texts_to_sequences(current_sequence_str)\n",
    "        if not tokenized_input or all(t == 0 for t in tokenized_input):\n",
    "            break\n",
    "        \n",
    "        input_tensor = torch.tensor(tokenized_input, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor).squeeze(0)\n",
    "        \n",
    "        if temperature > 0:\n",
    "            predictions = predictions / temperature\n",
    "        \n",
    "        probabilities = torch.softmax(predictions, dim=-1)\n",
    "        next_char_id = torch.multinomial(probabilities, 1).item()\n",
    "        generated_text += tokenizer.int_to_char.get(next_char_id, '')\n",
    "    return generated_text\n",
    "\n",
    "try:\n",
    "    tokenizer_general = load_object(os.path.join(BASE_OUTPUT_DIR_GENERAL, TOKENIZER_SUBDIR, 'char_tokenizer_general.pkl'))\n",
    "    total_chars_general = load_object(os.path.join(BASE_OUTPUT_DIR_GENERAL, TOKENIZER_SUBDIR, 'vocab_size_general_char.pkl'))\n",
    "    print(f\"Vocabulário de caracteres geral carregado. Tamanho: {total_chars_general}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Erro: Arquivos do tokenizer geral não encontrados. {e}\")\n",
    "    exit()\n",
    "\n",
    "MODEL_DROPOUT_STRATEGY_GENERAL = \"none\"\n",
    "MODEL_SAVE_FILEPATH_GENERAL = os.path.join(MODEL_DIR_BASE_GENERAL, f\"model_weights_general_strategy_{MODEL_DROPOUT_STRATEGY_GENERAL}_best.pt\")\n",
    "model_general = CharLSTM(total_chars_general, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT_RATE, MODEL_DROPOUT_STRATEGY_GENERAL).to(device)\n",
    "try:\n",
    "    model_general.load_state_dict(torch.load(MODEL_SAVE_FILEPATH_GENERAL, map_location=device))\n",
    "    model_general.eval()\n",
    "    print(f\"Modelo Geral carregado de: {MODEL_SAVE_FILEPATH_GENERAL}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Aviso: Modelo Geral não encontrado em '{MODEL_SAVE_FILEPATH_GENERAL}'. Será pulado na comparação.\")\n",
    "    model_general = None\n",
    "\n",
    "MODEL_DROPOUT_STRATEGY_FINE_TUNE = \"none\"\n",
    "MODEL_SAVE_FILEPATH_FINE_TUNE = os.path.join(MODEL_DIR_BASE_ARTIST_FINE_TUNED, CLEAN_TARGET_ARTIST_NAME, f\"model_weights_{CLEAN_TARGET_ARTIST_NAME}_finetuned_strategy_{MODEL_DROPOUT_STRATEGY_FINE_TUNE}_best.pt\")\n",
    "model_fine_tuned = CharLSTM(total_chars_general, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT_RATE, MODEL_DROPOUT_STRATEGY_FINE_TUNE).to(device)\n",
    "try:\n",
    "    model_fine_tuned.load_state_dict(torch.load(MODEL_SAVE_FILEPATH_FINE_TUNE, map_location=device))\n",
    "    model_fine_tuned.eval()\n",
    "    print(f\"Modelo Fine-tuned carregado de: {MODEL_SAVE_FILEPATH_FINE_TUNE}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Aviso: Modelo Fine-tuned não encontrado em '{MODEL_SAVE_FILEPATH_FINE_TUNE}'. Será pulado na comparação.\")\n",
    "    model_fine_tuned = None\n",
    "\n",
    "try:\n",
    "    ARTIST_LYRICS_CLEAN_ALL = []\n",
    "    df_target_artist_full = pd.read_csv(CSV_FILE_PATH_TARGET_ARTIST, usecols=['Lyric'])\n",
    "    for lyric_val in df_target_artist_full['Lyric']:\n",
    "        cleaned_lyric = clean_text(lyric_val)\n",
    "        if cleaned_lyric and PLACEHOLDER_PHRASE not in cleaned_lyric:\n",
    "            ARTIST_LYRICS_CLEAN_ALL.append(cleaned_lyric)\n",
    "    \n",
    "    random.seed(RANDOM_SEED)\n",
    "    random.shuffle(ARTIST_LYRICS_CLEAN_ALL)\n",
    "    num_test_artist_final = int(len(ARTIST_LYRICS_CLEAN_ALL) * TEST_SPLIT)\n",
    "    TEST_LYRICS_ARTIST_FINAL_FOR_GEN = ARTIST_LYRICS_CLEAN_ALL[:num_test_artist_final]\n",
    "    print(f\"\\nColetadas {len(TEST_LYRICS_ARTIST_FINAL_FOR_GEN)} músicas de '{TARGET_ARTIST}' para seleção de prompts.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar dados de teste do artista: {e}\")\n",
    "    TEST_LYRICS_ARTIST_FINAL_FOR_GEN = []\n",
    "\n",
    "# --- Geração e Comparação de Exemplos ---\n",
    "NUM_EXAMPLES = 10\n",
    "NUM_CHARS_TO_GENERATE_PER_EXAMPLE = 250\n",
    "GENERATION_TEMPERATURE = 0.7\n",
    "\n",
    "print(\"\\n--- Comparação de Geração de Texto (Modelo Geral vs. Fine-tuned) ---\")\n",
    "if not TEST_LYRICS_ARTIST_FINAL_FOR_GEN:\n",
    "    print(\"Não há músicas de teste para gerar prompts. Impossível prosseguir.\")\n",
    "else:\n",
    "    selected_lyric_indices = random.sample(range(len(TEST_LYRICS_ARTIST_FINAL_FOR_GEN)), min(NUM_EXAMPLES, len(TEST_LYRICS_ARTIST_FINAL_FOR_GEN)))\n",
    "    for i, lyric_idx in enumerate(selected_lyric_indices):\n",
    "        original_lyric = TEST_LYRICS_ARTIST_FINAL_FOR_GEN[lyric_idx]\n",
    "        \n",
    "        min_len_required = SEQUENCE_LENGTH + 20\n",
    "        if len(original_lyric) < min_len_required:\n",
    "            prompt_start_index = 0\n",
    "        else:\n",
    "            prompt_start_index = random.randint(0, len(original_lyric) - min_len_required)\n",
    "            \n",
    "        prompt = original_lyric[prompt_start_index : prompt_start_index + SEQUENCE_LENGTH]\n",
    "        real_continuation = original_lyric[prompt_start_index + SEQUENCE_LENGTH : prompt_start_index + SEQUENCE_LENGTH + NUM_CHARS_TO_GENERATE_PER_EXAMPLE]\n",
    "\n",
    "        print(f\"\\n--- Exemplo {i + 1} \" + \"=\"*65)\n",
    "        print(f\"PROMPT DE ENTRADA:\\n'{prompt}'\")\n",
    "        print(\"\\nCONTINUAÇÃO REAL:\")\n",
    "        print(f\"'{real_continuation}'\")\n",
    "\n",
    "        if model_general:\n",
    "            generated_general = generate_text_char_level(model_general, tokenizer_general, prompt, NUM_CHARS_TO_GENERATE_PER_EXAMPLE, GENERATION_TEMPERATURE)\n",
    "            print(\"\\nGERADO PELO MODELO GERAL:\")\n",
    "            print(f\"'{generated_general[SEQUENCE_LENGTH:]}'\")\n",
    "\n",
    "        if model_fine_tuned:\n",
    "            generated_fine_tuned = generate_text_char_level(model_fine_tuned, tokenizer_general, prompt, NUM_CHARS_TO_GENERATE_PER_EXAMPLE, GENERATION_TEMPERATURE)\n",
    "            print(\"\\nGERADO PELO MODELO FINE-TUNED:\")\n",
    "            print(f\"'{generated_fine_tuned[SEQUENCE_LENGTH:]}'\")\n",
    "        print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61527f79",
   "metadata": {},
   "source": [
    "## Geração do próximo caractere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4117003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Objeto carregado de: processed_data_general_char_split\\tokenizers\\char_tokenizer_general.pkl\n",
      "Objeto carregado de: processed_data_general_char_split\\tokenizers\\vocab_size_general_char.pkl\n",
      "Vocabulário de caracteres GERAL carregado. Tamanho do vocabulário: 36\n",
      "Modelo Geral carregado de: models_general_char_split\\model_weights_general_strategy_none_best.pt\n",
      "Modelo Fine-tuned carregado de: models_by_artist_char_split_finetuned\\arianagrande\\model_weights_arianagrande_finetuned_strategy_none_best.pt\n",
      "Dataset de teste do artista carregado com 42190 amostras.\n",
      "\n",
      "--- Comparação de Predições de Próximo Caractere (10 Exemplos) ---\n",
      "Usando prompts do conjunto de TESTE EXCLUSIVO do artista: ArianaGrande\n",
      "================================================================================\n",
      "\n",
      "--- Exemplo 1 ---\n",
      "  Input (últimos 30 caracteres do prompt): 'ss stole you wouldn't let anyb'\n",
      "  Caracter Real Seguinte:         'o'\n",
      "  Previsto (Modelo Geral):        'o' (Acertou)\n",
      "  Previsto (Modelo Fine-tuned):   'o' (Acertou)\n",
      "------------------------------\n",
      "\n",
      "--- Exemplo 2 ---\n",
      "  Input (últimos 30 caracteres do prompt): 'benched aww your bum knee now '\n",
      "  Caracter Real Seguinte:         'i'\n",
      "  Previsto (Modelo Geral):        'i' (Acertou)\n",
      "  Previsto (Modelo Fine-tuned):   'i' (Acertou)\n",
      "------------------------------\n",
      "\n",
      "--- Exemplo 3 ---\n",
      "  Input (últimos 30 caracteres do prompt): 'me yeah pre ariana grande fall'\n",
      "  Caracter Real Seguinte:         'i'\n",
      "  Previsto (Modelo Geral):        ' ' (Errou)\n",
      "  Previsto (Modelo Fine-tuned):   'i' (Acertou)\n",
      "------------------------------\n",
      "\n",
      "--- Exemplo 4 ---\n",
      "  Input (últimos 30 caracteres do prompt): ' my side hold me close under t'\n",
      "  Caracter Real Seguinte:         'h'\n",
      "  Previsto (Modelo Geral):        'h' (Acertou)\n",
      "  Previsto (Modelo Fine-tuned):   'h' (Acertou)\n",
      "------------------------------\n",
      "\n",
      "--- Exemplo 5 ---\n",
      "  Input (últimos 30 caracteres do prompt): 'keep i keep on breathin' just '\n",
      "  Caracter Real Seguinte:         'k'\n",
      "  Previsto (Modelo Geral):        'l' (Errou)\n",
      "  Previsto (Modelo Fine-tuned):   'k' (Acertou)\n",
      "------------------------------\n",
      "\n",
      "--- Exemplo 6 ---\n",
      "  Input (últimos 30 caracteres do prompt): 'u baby boy you got all my all '\n",
      "  Caracter Real Seguinte:         'm'\n",
      "  Previsto (Modelo Geral):        'a' (Errou)\n",
      "  Previsto (Modelo Fine-tuned):   'i' (Errou)\n",
      "------------------------------\n",
      "\n",
      "--- Exemplo 7 ---\n",
      "  Input (últimos 30 caracteres do prompt): 'eah you drink it just like wat'\n",
      "  Caracter Real Seguinte:         'e'\n",
      "  Previsto (Modelo Geral):        'c' (Errou)\n",
      "  Previsto (Modelo Fine-tuned):   'e' (Acertou)\n",
      "------------------------------\n",
      "\n",
      "--- Exemplo 8 ---\n",
      "  Input (últimos 30 caracteres do prompt): 'erything the darkness stole yo'\n",
      "  Caracter Real Seguinte:         'u'\n",
      "  Previsto (Modelo Geral):        'u' (Acertou)\n",
      "  Previsto (Modelo Fine-tuned):   'u' (Acertou)\n",
      "------------------------------\n",
      "\n",
      "--- Exemplo 9 ---\n",
      "  Input (últimos 30 caracteres do prompt): ' just keep breathin' and breat'\n",
      "  Caracter Real Seguinte:         'h'\n",
      "  Previsto (Modelo Geral):        'h' (Acertou)\n",
      "  Previsto (Modelo Fine-tuned):   'h' (Acertou)\n",
      "------------------------------\n",
      "\n",
      "--- Exemplo 10 ---\n",
      "  Input (últimos 30 caracteres do prompt): 'ou wouldn't let anybody speak '\n",
      "  Caracter Real Seguinte:         'a'\n",
      "  Previsto (Modelo Geral):        't' (Errou)\n",
      "  Previsto (Modelo Fine-tuned):   'a' (Acertou)\n",
      "------------------------------\n",
      "\n",
      "Comparação de predição de caracteres concluída.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Realiza uma comparação direta da predição do próximo caractere entre o modelo\n",
    "geral e o modelo fine-tuned. A célula carrega o tokenizador, ambos os modelos\n",
    "treinados e o conjunto de teste exclusivo do artista. Em seguida, seleciona\n",
    "exemplos aleatórios deste conjunto de teste e, para cada um, exibe o prompt,\n",
    "o caractere real seguinte e a predição de cada modelo, permitindo uma análise\n",
    "qualitativa e quantitativa da melhoria na previsão de caracteres após o\n",
    "fine-tuning.\n",
    "\"\"\"\n",
    "print(\"\\n--- Comparação de Predição de Próximo Caractere ---\")\n",
    "try:\n",
    "    tokenizer_general = load_object(os.path.join(BASE_OUTPUT_DIR_GENERAL, TOKENIZER_SUBDIR, 'char_tokenizer_general.pkl'))\n",
    "    total_chars_general = load_object(os.path.join(BASE_OUTPUT_DIR_GENERAL, TOKENIZER_SUBDIR, 'vocab_size_general_char.pkl'))\n",
    "    print(f\"Vocabulário de caracteres geral carregado. Tamanho: {total_chars_general}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Erro: Arquivos do tokenizer geral não encontrados. {e}\")\n",
    "    exit()\n",
    "\n",
    "MODEL_DROPOUT_STRATEGY = \"none\" # Estratégia usada para ambos os modelos\n",
    "MODEL_SAVE_FILEPATH_GENERAL = os.path.join(MODEL_DIR_BASE_GENERAL, f\"model_weights_general_strategy_{MODEL_DROPOUT_STRATEGY}_best.pt\")\n",
    "MODEL_SAVE_FILEPATH_FINE_TUNE = os.path.join(MODEL_DIR_BASE_ARTIST_FINE_TUNED, CLEAN_TARGET_ARTIST_NAME, f\"model_weights_{CLEAN_TARGET_ARTIST_NAME}_finetuned_strategy_{MODEL_DROPOUT_STRATEGY}_best.pt\")\n",
    "\n",
    "model_general = CharLSTM(total_chars_general, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT_RATE, MODEL_DROPOUT_STRATEGY).to(device)\n",
    "try:\n",
    "    model_general.load_state_dict(torch.load(MODEL_SAVE_FILEPATH_GENERAL, map_location=device))\n",
    "    model_general.eval()\n",
    "    print(f\"Modelo Geral carregado de: {MODEL_SAVE_FILEPATH_GENERAL}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Aviso: Modelo Geral não encontrado em '{MODEL_SAVE_FILEPATH_GENERAL}'.\")\n",
    "    model_general = None\n",
    "\n",
    "model_fine_tuned = CharLSTM(total_chars_general, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT_RATE, MODEL_DROPOUT_STRATEGY).to(device)\n",
    "try:\n",
    "    model_fine_tuned.load_state_dict(torch.load(MODEL_SAVE_FILEPATH_FINE_TUNE, map_location=device))\n",
    "    model_fine_tuned.eval()\n",
    "    print(f\"Modelo Fine-tuned carregado de: {MODEL_SAVE_FILEPATH_FINE_TUNE}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Aviso: Modelo Fine-tuned não encontrado em '{MODEL_SAVE_FILEPATH_FINE_TUNE}'.\")\n",
    "    model_fine_tuned = None\n",
    "\n",
    "X_test_artist_path = os.path.join(BASE_OUTPUT_DIR_ARTIST, CLEAN_TARGET_ARTIST_NAME, DATA_SUBDIR, f'X_test_{CLEAN_TARGET_ARTIST_NAME}_char.npy')\n",
    "y_test_artist_path = os.path.join(BASE_OUTPUT_DIR_ARTIST, CLEAN_TARGET_ARTIST_NAME, DATA_SUBDIR, f'y_test_{CLEAN_TARGET_ARTIST_NAME}_char.npy')\n",
    "try:\n",
    "    test_dataset_artist = LyricsDataset(X_test_artist_path, y_test_artist_path)\n",
    "    print(f\"Dataset de teste do artista carregado com {len(test_dataset_artist)} amostras.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Erro: Dados de teste do artista não encontrados. {e}\")\n",
    "    test_dataset_artist = None\n",
    "\n",
    "if test_dataset_artist and (model_general or model_fine_tuned):\n",
    "    num_examples_to_show = 10\n",
    "    random_indices = random.sample(range(len(test_dataset_artist)), min(num_examples_to_show, len(test_dataset_artist)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(random_indices):\n",
    "            input_sequence, actual_next_char_id = test_dataset_artist[idx]\n",
    "            input_sequence_batch = input_sequence.unsqueeze(0).to(device)\n",
    "            \n",
    "            predicted_char_general = \"[Não Carregado]\"\n",
    "            if model_general:\n",
    "                output_general = model_general(input_sequence_batch)\n",
    "                _, predicted_id_general = torch.max(output_general, 1)\n",
    "                predicted_char_general = tokenizer_general.sequences_to_texts([predicted_id_general.item()])\n",
    "\n",
    "            predicted_char_fine_tuned = \"[Não Carregado]\"\n",
    "            if model_fine_tuned:\n",
    "                output_fine_tuned = model_fine_tuned(input_sequence_batch)\n",
    "                _, predicted_id_fine_tuned = torch.max(output_fine_tuned, 1)\n",
    "                predicted_char_fine_tuned = tokenizer_general.sequences_to_texts([predicted_id_fine_tuned.item()])\n",
    "\n",
    "            input_text = tokenizer_general.sequences_to_texts(input_sequence.tolist())\n",
    "            actual_char = tokenizer_general.sequences_to_texts([actual_next_char_id.item()])\n",
    "\n",
    "            print(f\"\\n--- Exemplo {i+1} ---\")\n",
    "            print(f\"  Prompt (últimos 40 chars): '...{input_text[-40:]}'\")\n",
    "            print(f\"  Caracter Real:                '{actual_char}'\")\n",
    "            print(f\"  Previsto (Modelo Geral):      '{predicted_char_general}' {'(Acertou)' if actual_char == predicted_char_general else ''}\")\n",
    "            print(f\"  Previsto (Modelo Fine-tuned): '{predicted_char_fine_tuned}' {'(Acertou)' if actual_char == predicted_char_fine_tuned else ''}\")\n",
    "\n",
    "    print(\"\\nComparação de predição de caracteres concluída.\")\n",
    "else:\n",
    "    print(\"\\nComparação não pôde ser executada devido à falta do dataset de teste ou de ambos os modelos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58258eb",
   "metadata": {},
   "source": [
    "## Geração de letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ecc1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Objeto carregado de: processed_data_general_char_split\\tokenizers\\char_tokenizer_general.pkl\n",
      "Objeto carregado de: processed_data_general_char_split\\tokenizers\\vocab_size_general_char.pkl\n",
      "Vocabulário de caracteres GERAL carregado. Tamanho do vocabulário: 36\n",
      "Modelo fine-tuned de 'ArianaGrande' carregado de: models_by_artist_char_split_finetuned\\arianagrande\\model_weights_arianagrande_finetuned_strategy_none_best.pt\n",
      "\n",
      "--- INICIANDO GERAÇÃO DE LETRA DE MÚSICA ---\n",
      "Artista: ArianaGrande\n",
      "Prompt: 'don't want nobody else around me just need you right here you're like the only thing that i see it's'\n",
      "Caracteres a gerar: 120\n",
      "Temperatura: 0\n",
      "================================================================================\n",
      "Iniciando geração com seed (ajustada para 100 caracteres): 'don't want nobody else around me just need you right here you're like the only thing that i see it's'\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando caracteres: 100%|██████████| 120/120 [00:02<00:00, 58.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- APENAS O TEXTO NOVO GERADO ---\n",
      " love i just want to break your heart right back yeah all this time i was blind running 'round telling everybody my baby\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define uma função para geração de texto e a utiliza para criar uma nova\n",
    "letra de música a partir de um prompt inicial. Esta célula assume que os\n",
    "modelos e o tokenizador estão disponíveis para serem carregados e que as\n",
    "configurações base estão definidas. Ela carrega o modelo fine-tuned,\n",
    "define um prompt e parâmetros de geração (temperatura, comprimento) e\n",
    "exibe o texto resultante.\n",
    "\"\"\"\n",
    "def generate_text(model, tokenizer, seed_text, num_generate=500, temperature=0.8, sequence_length=SEQUENCE_LENGTH, device=device):\n",
    "    \"\"\"Gera texto caractere a caractere a partir de uma seed, controlando a criatividade com a temperatura.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    processed_seed_text = clean_text(seed_text)\n",
    "    if len(processed_seed_text) < sequence_length:\n",
    "        processed_seed_text = ' ' * (sequence_length - len(processed_seed_text)) + processed_seed_text\n",
    "    elif len(processed_seed_text) > sequence_length:\n",
    "        processed_seed_text = processed_seed_text[-sequence_length:]\n",
    "\n",
    "    generated_text = processed_seed_text\n",
    "    print(f\"Iniciando geração com seed: '{generated_text}'\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for _ in tqdm(range(num_generate), desc=\"Gerando caracteres\"):\n",
    "        current_sequence_str = generated_text[-sequence_length:]\n",
    "        tokenized_input = tokenizer.texts_to_sequences(current_sequence_str)\n",
    "        \n",
    "        if not tokenized_input or all(t == 0 for t in tokenized_input):\n",
    "            print(\"\\n[Aviso: Sequência de input inválida, encerrando geração.]\")\n",
    "            break\n",
    "\n",
    "        input_tensor = torch.tensor(tokenized_input, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor).squeeze(0)\n",
    "        \n",
    "        if temperature <= 0:\n",
    "            next_char_id = torch.argmax(predictions, dim=-1).item()\n",
    "        else:\n",
    "            predictions = predictions / temperature\n",
    "            probabilities = torch.softmax(predictions, dim=-1)\n",
    "            next_char_id = torch.multinomial(probabilities, 1).item()\n",
    "\n",
    "        next_char = tokenizer.int_to_char.get(next_char_id, '')\n",
    "        generated_text += next_char\n",
    "    return generated_text\n",
    "\n",
    "TOKENIZER_PATH_GENERAL = os.path.join(BASE_OUTPUT_DIR_GENERAL, TOKENIZER_SUBDIR, 'char_tokenizer_general.pkl')\n",
    "VOCAB_SIZE_PATH_GENERAL = os.path.join(BASE_OUTPUT_DIR_GENERAL, TOKENIZER_SUBDIR, 'vocab_size_general_char.pkl')\n",
    "try:\n",
    "    tokenizer = load_object(TOKENIZER_PATH_GENERAL)\n",
    "    total_chars = load_object(VOCAB_SIZE_PATH_GENERAL)\n",
    "    print(f\"Vocabulário geral carregado. Tamanho: {total_chars}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "MODEL_DROPOUT_STRATEGY = \"none\"\n",
    "MODEL_SAVE_FILEPATH_FINE_TUNED = os.path.join(MODEL_DIR_BASE_ARTIST_FINE_TUNED, CLEAN_TARGET_ARTIST_NAME, f\"model_weights_{CLEAN_TARGET_ARTIST_NAME}_finetuned_strategy_{MODEL_DROPOUT_STRATEGY}_best.pt\")\n",
    "\n",
    "if not os.path.exists(MODEL_SAVE_FILEPATH_FINE_TUNED):\n",
    "    print(f\"Erro: Modelo fine-tuned não encontrado em '{MODEL_SAVE_FILEPATH_FINE_TUNED}'.\")\n",
    "    exit()\n",
    "\n",
    "model = CharLSTM(\n",
    "    vocab_size=total_chars,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    dropout_strategy=MODEL_DROPOUT_STRATEGY\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_FILEPATH_FINE_TUNED, map_location=device))\n",
    "print(f\"Modelo fine-tuned de '{TARGET_ARTIST}' carregado de: {MODEL_SAVE_FILEPATH_FINE_TUNED}\")\n",
    "\n",
    "prompt_inicial = \"don't want nobody else around me just need you right here you're like the only thing that i see it's\"\n",
    "num_chars_to_generate = 500\n",
    "generation_temperature = 0.7\n",
    "\n",
    "print(\"\\n--- INICIANDO GERAÇÃO DE LETRA DE MÚSICA ---\")\n",
    "generated_lyric = generate_text(model, tokenizer, prompt_inicial, num_chars_to_generate, generation_temperature)\n",
    "\n",
    "print(\"\\n--- TEXTO GERADO ---\")\n",
    "print(generated_lyric[SEQUENCE_LENGTH:])\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LyricGen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
